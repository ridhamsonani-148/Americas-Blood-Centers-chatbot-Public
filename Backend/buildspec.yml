version: 0.2

phases:
  install:
    runtime-versions:
      nodejs: 20
      python: 3.11
    commands:
      - echo "Installing AWS CDK CLI..."
      - npm install -g aws-cdk@latest
      - cd Backend
      - npm install

  pre_build:
    commands:
      - echo "=== Building TypeScript ==="
      - npm run build
      - echo "‚úÖ TypeScript build completed"
      - echo "=== Installing Lambda dependencies ==="
      - cd lambda && pip install -r requirements.txt -t . && cd ..
      - echo "‚úÖ Lambda dependencies installed"
      - echo "=== Bootstrapping CDK Environment ==="
      - cdk bootstrap --require-approval never
      - echo "‚úÖ CDK Bootstrap completed"

  build:
    commands:
      - |
        if [ "$ACTION" = "destroy" ]; then
          echo "=== Destroying Stack ==="
          cdk destroy AmericasBloodCentersBedrockStack --force \
            --context projectName="$PROJECT_NAME" \
            --context modelId="$MODEL_ID" \
            --context embeddingModelId="$EMBEDDING_MODEL_ID"
          echo "‚úÖ Stack destroyed successfully"
        else
          echo "========================================="
          echo "Deploying America's Blood Centers Bedrock Chatbot"
          echo "========================================="
          
          echo "=== PHASE 1: CDK Infrastructure Deployment ==="
          cdk deploy AmericasBloodCentersBedrockStack --require-approval never \
            --context projectName="$PROJECT_NAME" \
            --context modelId="$MODEL_ID" \
            --context embeddingModelId="$EMBEDDING_MODEL_ID" \
            --outputs-file outputs.json
          
          if [ $? -ne 0 ]; then
            echo "‚ùå ERROR: CDK deployment failed"
            exit 1
          fi
          
          echo "‚úÖ CDK deployment successful"
          
          echo "=== PHASE 2: Extracting CDK Outputs ==="
          DOCUMENTS_BUCKET=$(cat outputs.json | jq -r '.AmericasBloodCentersBedrockStack.DocumentsBucketName // empty')
          KB_ROLE_ARN=$(cat outputs.json | jq -r '.AmericasBloodCentersBedrockStack.KnowledgeBaseRoleArn // empty')
          CHAT_LAMBDA_NAME=$(cat outputs.json | jq -r '.AmericasBloodCentersBedrockStack.ChatLambdaFunctionName // empty')
          DATA_INGESTION_LAMBDA_NAME=$(cat outputs.json | jq -r '.AmericasBloodCentersBedrockStack.DataIngestionFunctionName // empty')
          API_URL=$(cat outputs.json | jq -r '.AmericasBloodCentersBedrockStack.ApiGatewayUrl // empty')
          AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
          
          if [ -z "$DOCUMENTS_BUCKET" ] || [ -z "$KB_ROLE_ARN" ] || [ -z "$CHAT_LAMBDA_NAME" ]; then
            echo "‚ùå ERROR: Failed to extract required outputs from CDK deployment"
            echo "Documents Bucket: $DOCUMENTS_BUCKET"
            echo "KB Role ARN: $KB_ROLE_ARN"
            echo "Chat Lambda: $CHAT_LAMBDA_NAME"
            exit 1
          fi
          
          echo "‚úÖ Outputs extracted successfully"
          echo "  üìÅ Documents Bucket: $DOCUMENTS_BUCKET"
          echo "  üîê KB Role ARN: $KB_ROLE_ARN"
          echo "  üí¨ Chat Lambda: $CHAT_LAMBDA_NAME"
          echo "  üìä Data Ingestion Lambda: $DATA_INGESTION_LAMBDA_NAME"
          echo "  üåê API Gateway URL: $API_URL"
          echo ""
          echo "========================================="
          echo "=== PHASE 3: Knowledge Base Resources ==="
          echo "========================================="
          
          # Step 3: Create OpenSearch Serverless Collection
          echo "Step 3: Creating OpenSearch Serverless collection..."
          # Add timestamp to ensure unique collection name
          TIMESTAMP=$(date +%s)
          COLLECTION_NAME="${PROJECT_NAME}-kb-${TIMESTAMP}"
          echo "üîç Debug: PROJECT_NAME='$PROJECT_NAME'"
          echo "üîç Debug: COLLECTION_NAME='$COLLECTION_NAME'"
          echo "üîç Debug: Collection name length: ${#COLLECTION_NAME}"
          
          # Always create a new collection with unique name to avoid permission issues
          echo "Creating new OpenSearch Serverless collection with proper permissions..."
            
            # Create network security policy (only if it doesn't exist)
            echo "Creating network security policy..."
            if ! aws opensearchserverless get-security-policy --name "${PROJECT_NAME}-network-policy" --type network --region "$CDK_DEFAULT_REGION" >/dev/null 2>&1; then
              aws opensearchserverless create-security-policy \
                --name "${PROJECT_NAME}-network-policy" \
                --type network \
                --policy '[{"Rules":[{"Resource":["collection/'$COLLECTION_NAME'"],"ResourceType":"collection"}],"AllowFromPublic":true}]' \
                --region "$CDK_DEFAULT_REGION" && echo "‚úì Network security policy created"
            else
              echo "‚úì Network security policy already exists"
            fi
            
            # Create encryption security policy (only if it doesn't exist)
            echo "Creating encryption security policy..."
            if ! aws opensearchserverless get-security-policy --name "${PROJECT_NAME}-encryption-policy" --type encryption --region "$CDK_DEFAULT_REGION" >/dev/null 2>&1; then
              aws opensearchserverless create-security-policy \
                --name "${PROJECT_NAME}-encryption-policy" \
                --type encryption \
                --policy '{"Rules":[{"Resource":["collection/'$COLLECTION_NAME'"],"ResourceType":"collection"}],"AWSOwnedKey":true}' \
                --region "$CDK_DEFAULT_REGION" && echo "‚úì Encryption security policy created"
            else
              echo "‚úì Encryption security policy already exists"
            fi
            
            # Create data access policy (only if it doesn't exist)
            echo "Creating data access policy..."
            if ! aws opensearchserverless get-access-policy --name "${PROJECT_NAME}-data-access-policy" --type data --region "$CDK_DEFAULT_REGION" >/dev/null 2>&1; then
              # Get CodeBuild role ARN for access policy
              CODEBUILD_ROLE_ARN=$(aws sts get-caller-identity --query Arn --output text | sed 's/:user\//:role\//' | sed 's/\/[^\/]*$/\/codebuild-service-role/')
              echo "üîç Debug: CodeBuild role ARN: $CODEBUILD_ROLE_ARN"
              
              # Note: We'll add the Knowledge Base role after it's created
              aws opensearchserverless create-access-policy \
                --name "${PROJECT_NAME}-data-access-policy" \
                --type data \
                --policy '[{"Rules":[{"Resource":["collection/'$COLLECTION_NAME'"],"Permission":["aoss:CreateCollectionItems","aoss:DeleteCollectionItems","aoss:UpdateCollectionItems","aoss:DescribeCollectionItems"],"ResourceType":"collection"},{"Resource":["index/'$COLLECTION_NAME'/*"],"Permission":["aoss:CreateIndex","aoss:DeleteIndex","aoss:UpdateIndex","aoss:DescribeIndex","aoss:ReadDocument","aoss:WriteDocument"],"ResourceType":"index"}],"Principal":["arn:aws:iam::'$AWS_ACCOUNT_ID':root","'$CODEBUILD_ROLE_ARN'"]}]' \
                --region "$CDK_DEFAULT_REGION" && echo "‚úì Data access policy created (will update with KB role later)"
            else
              echo "‚úì Data access policy already exists"
            fi
            
            # Wait for policies to propagate
            echo "Waiting for policies to propagate..."
            sleep 15
            
            # Create collection
            echo "Creating OpenSearch Serverless collection..."
            COLLECTION_CREATE_OUTPUT=$(aws opensearchserverless create-collection \
              --name "$COLLECTION_NAME" \
              --type VECTORSEARCH \
              --description "Vector collection for America's Blood Centers knowledge base" \
              --region "$CDK_DEFAULT_REGION" 2>&1)
            
            if [ $? -eq 0 ]; then
              COLLECTION_ARN=$(echo "$COLLECTION_CREATE_OUTPUT" | jq -r '.createCollectionDetail.arn // empty')
              echo "‚úì Collection creation initiated: $COLLECTION_ARN"
            else
              echo "‚úó Failed to create OpenSearch collection"
              echo "Error: $COLLECTION_CREATE_OUTPUT"
              exit 1
            fi
          fi
          
          # Step 4: Wait for collection to be active
          echo "Step 4: Waiting for collection to be active..."
          MAX_WAIT=900  # Increased to 15 minutes
          ELAPSED=0
          while [ $ELAPSED -lt $MAX_WAIT ]; do
            STATUS=$(aws opensearchserverless list-collections \
              --region "$CDK_DEFAULT_REGION" \
              --query "collectionSummaries[?name=='$COLLECTION_NAME'].status | [0]" \
              --output text 2>/dev/null || echo "ERROR")
            
            echo "  Collection status: $STATUS (waited ${ELAPSED}s/${MAX_WAIT}s)"
            
            if [ "$STATUS" = "ACTIVE" ]; then
              echo "‚úì Collection is active"
              # Get the final collection ARN
              COLLECTION_ARN=$(aws opensearchserverless list-collections \
                --region "$CDK_DEFAULT_REGION" \
                --query "collectionSummaries[?name=='$COLLECTION_NAME'].arn | [0]" \
                --output text)
              echo "  Final Collection ARN: $COLLECTION_ARN"
              break
            elif [ "$STATUS" = "FAILED" ]; then
              echo "‚úó Collection creation failed"
              # Get collection details for debugging
              aws opensearchserverless list-collections \
                --region "$CDK_DEFAULT_REGION" \
                --query "collectionSummaries[?name=='$COLLECTION_NAME']" \
                --output json
              exit 1
            elif [ "$STATUS" = "ERROR" ] || [ -z "$STATUS" ]; then
              echo "‚úó Failed to get collection status"
              exit 1
            fi
            
            sleep 30
            ELAPSED=$((ELAPSED + 30))
          done
          
          if [ $ELAPSED -ge $MAX_WAIT ]; then
            echo "‚úó Timeout waiting for collection to be active after ${MAX_WAIT} seconds"
            echo "Current status: $STATUS"
            exit 1
          fi
          
          # Step 4.5: Update data access policy with Knowledge Base role
          echo "Step 4.5: Adding Knowledge Base role to OpenSearch data access policy..."
          
          # Extract just the role name from the full ARN
          KB_ROLE_NAME=$(echo "$KB_ROLE_ARN" | sed 's/.*role\///')
          echo "üîç Debug: KB Role Name: $KB_ROLE_NAME"
          echo "üîç Debug: KB Role ARN: $KB_ROLE_ARN"
          
          # Get current policy version
          echo "Getting current data access policy version..."
          POLICY_VERSION=$(aws opensearchserverless get-access-policy \
            --name "${PROJECT_NAME}-data-access-policy" \
            --type data \
            --region "$CDK_DEFAULT_REGION" \
            --query 'accessPolicyDetail.policyVersion' \
            --output text)
          
          echo "üîç Debug: Current policy version: $POLICY_VERSION"
          
          # Update the data access policy to include the Knowledge Base role
          echo "Updating data access policy to include Knowledge Base role..."
          aws opensearchserverless update-access-policy \
            --name "${PROJECT_NAME}-data-access-policy" \
            --type data \
            --policy-version "$POLICY_VERSION" \
            --policy '[{"Rules":[{"Resource":["collection/'$COLLECTION_NAME'"],"Permission":["aoss:CreateCollectionItems","aoss:DeleteCollectionItems","aoss:UpdateCollectionItems","aoss:DescribeCollectionItems"],"ResourceType":"collection"},{"Resource":["index/'$COLLECTION_NAME'/*"],"Permission":["aoss:CreateIndex","aoss:DeleteIndex","aoss:UpdateIndex","aoss:DescribeIndex","aoss:ReadDocument","aoss:WriteDocument"],"ResourceType":"index"}],"Principal":["arn:aws:iam::'$AWS_ACCOUNT_ID':root","'$KB_ROLE_ARN'"]}]' \
            --region "$CDK_DEFAULT_REGION" && echo "‚úì Data access policy updated with Knowledge Base role"
          
          # Wait for policy update to propagate
          echo "Waiting for policy update to propagate..."
          sleep 10
          
          # Step 4.6: Create the required vector index manually
          echo "Step 4.6: Creating vector index in OpenSearch collection..."
          
          # Get the collection ID from the ARN
          COLLECTION_ID=$(echo "$COLLECTION_ARN" | sed 's/.*collection\///')
          echo "üîç Debug: Collection ID extracted: $COLLECTION_ID"
          
          # Create the index using AWS CLI (now that we have proper permissions)
          echo "Creating bedrock-knowledge-base-default-index..."
          
          INDEX_CREATION_OUTPUT=$(aws opensearchserverless create-index \
            --region "$CDK_DEFAULT_REGION" \
            --id "$COLLECTION_ID" \
            --index-name "bedrock-knowledge-base-default-index" \
            --index-schema '{
              "settings": {
                "index.knn": true
              },
              "mappings": {
                "properties": {
                  "bedrock-knowledge-base-default-vector": {
                    "type": "knn_vector",
                    "dimension": 1536,
                    "method": {
                      "name": "hnsw",
                      "engine": "faiss",
                      "parameters": {
                        "ef_construction": 512,
                        "m": 16
                      }
                    }
                  },
                  "AMAZON_BEDROCK_TEXT_CHUNK": {
                    "type": "text"
                  },
                  "AMAZON_BEDROCK_METADATA": {
                    "type": "text"
                  }
                }
              }
            }' 2>&1)
          
          if echo "$INDEX_CREATION_OUTPUT" | grep -q "indexDetail"; then
            echo "‚úì Vector index created successfully"
            echo "Index details: $INDEX_CREATION_OUTPUT"
          elif echo "$INDEX_CREATION_OUTPUT" | grep -q "already exists\|ResourceAlreadyExistsException"; then
            echo "‚úì Index already exists, continuing..."
          else
            echo "‚ö† Index creation response: $INDEX_CREATION_OUTPUT"
            echo "‚ö† Index creation may have failed, but continuing..."
          fi
          
          # Wait for index to be ready
          echo "Waiting for index to be ready..."
          sleep 15
          
          echo "‚úì Collection and index are ready for Knowledge Base creation"
          
          # Step 5: Create Knowledge Base
          echo "Step 5: Creating Bedrock Knowledge Base..."
          KB_ID=$(aws bedrock-agent list-knowledge-bases \
            --region "$CDK_DEFAULT_REGION" \
            --query "knowledgeBaseSummaries[?name=='${PROJECT_NAME}-knowledge-base'].knowledgeBaseId | [0]" \
            --output text 2>/dev/null || echo "")
          
          if [ -n "$KB_ID" ] && [ "$KB_ID" != "None" ] && [ "$KB_ID" != "null" ]; then
            echo "‚úì Found existing Knowledge Base: $KB_ID"
          else
            echo "Creating new Bedrock Knowledge Base..."
            KB_CREATE_OUTPUT=$(aws bedrock-agent create-knowledge-base \
              --name "${PROJECT_NAME}-knowledge-base" \
              --description "Knowledge base for America's Blood Centers chatbot" \
              --role-arn "$KB_ROLE_ARN" \
              --knowledge-base-configuration '{
                "type": "VECTOR",
                "vectorKnowledgeBaseConfiguration": {
                  "embeddingModelArn": "arn:aws:bedrock:'"$CDK_DEFAULT_REGION"'::foundation-model/'"$EMBEDDING_MODEL_ID"'"
                }
              }' \
              --storage-configuration '{
                "type": "OPENSEARCH_SERVERLESS",
                "opensearchServerlessConfiguration": {
                  "collectionArn": "'"$COLLECTION_ARN"'",
                  "vectorIndexName": "bedrock-knowledge-base-default-index",
                  "fieldMapping": {
                    "vectorField": "bedrock-knowledge-base-default-vector",
                    "textField": "AMAZON_BEDROCK_TEXT_CHUNK",
                    "metadataField": "AMAZON_BEDROCK_METADATA"
                  }
                }
              }' \
              --region "$CDK_DEFAULT_REGION" 2>&1)
            
            if [ $? -eq 0 ]; then
              KB_ID=$(echo "$KB_CREATE_OUTPUT" | jq -r '.knowledgeBase.knowledgeBaseId // empty')
              echo "‚úì Knowledge Base created: $KB_ID"
            else
              echo "‚úó Failed to create Knowledge Base"
              echo "Error: $KB_CREATE_OUTPUT"
              echo "Collection ARN: $COLLECTION_ARN"
              echo "KB Role ARN: $KB_ROLE_ARN"
              exit 1
            fi
          fi
          
          # Step 6: Create Data Sources (S3 for PDFs + Web Crawler for Websites)
          echo "=== PHASE 4: Creating Dual Data Sources ==="
          BUCKET_ARN="arn:aws:s3:::${DOCUMENTS_BUCKET}"
          
          # Data Source 1: S3 for PDFs with Bedrock Data Automation Parser
          echo "Creating S3 Data Source for PDFs..."
          S3_DS_ID=$(aws bedrock-agent list-data-sources \
            --knowledge-base-id "$KB_ID" \
            --region "$CDK_DEFAULT_REGION" \
            --query "dataSourceSummaries[?name=='blood-centers-pdfs'].dataSourceId | [0]" \
            --output text 2>/dev/null || echo "")
          
          if [ -n "$S3_DS_ID" ] && [ "$S3_DS_ID" != "None" ] && [ "$S3_DS_ID" != "null" ]; then
            echo "‚úì Found existing S3 Data Source: $S3_DS_ID"
          else
            echo "Creating new S3 Data Source for PDFs..."
            S3_DS_ID=$(aws bedrock-agent create-data-source \
              --name "blood-centers-pdfs" \
              --description "PDF documents about blood donation with Bedrock Data Automation parser" \
              --knowledge-base-id "$KB_ID" \
              --data-source-configuration '{
                "type": "S3",
                "s3Configuration": {
                  "bucketArn": "'"$BUCKET_ARN"'",
                  "inclusionPrefixes": ["pdfs/"]
                }
              }' \
              --vector-ingestion-configuration '{
                "chunkingConfiguration": {
                  "chunkingStrategy": "FIXED_SIZE",
                  "fixedSizeChunkingConfiguration": {
                    "maxTokens": 300,
                    "overlapPercentage": 20
                  }
                },
                "parsingConfiguration": {
                  "parsingStrategy": "BEDROCK_DATA_AUTOMATION",
                  "bedrockDataAutomationConfiguration": {
                    "parsingPrompt": "Extract all text content from PDF documents, preserving structure and formatting. Focus on blood donation information, statistics, guidelines, and eligibility criteria."
                  }
                }
              }' \
              --region "$CDK_DEFAULT_REGION" \
              --query 'dataSource.dataSourceId' --output text)
            
            if [ -z "$S3_DS_ID" ] || [ "$S3_DS_ID" = "None" ]; then
              echo "‚úó Failed to create S3 Data Source"
              exit 1
            fi
            
            echo "‚úì S3 Data Source created: $S3_DS_ID"
          fi
          
          # Data Source 2: Web Crawler for Websites
          echo "Creating Web Crawler Data Source..."
          WEB_DS_ID=$(aws bedrock-agent list-data-sources \
            --knowledge-base-id "$KB_ID" \
            --region "$CDK_DEFAULT_REGION" \
            --query "dataSourceSummaries[?name=='blood-centers-websites'].dataSourceId | [0]" \
            --output text 2>/dev/null || echo "")
          
          if [ -n "$WEB_DS_ID" ] && [ "$WEB_DS_ID" != "None" ] && [ "$WEB_DS_ID" != "null" ]; then
            echo "‚úì Found existing Web Crawler Data Source: $WEB_DS_ID"
          else
            echo "Creating new Web Crawler Data Source..."
            WEB_DS_ID=$(aws bedrock-agent create-data-source \
              --name "blood-centers-websites" \
              --description "Blood donation websites and news from America's Blood Centers" \
              --knowledge-base-id "$KB_ID" \
              --data-source-configuration '{
                "type": "WEB",
                "webConfiguration": {
                  "urlConfiguration": {
                    "seedUrls": [
                      {"url": "https://americasblood.org/for-donors/americas-blood-supply/"},
                      {"url": "https://americasblood.org/for-donors/find-a-blood-center/"},
                      {"url": "https://americasblood.org/news/"},
                      {"url": "https://americasblood.org/newsroom/"},
                      {"url": "https://americasblood.org/one-pagers-faqs/"}
                    ]
                  },
                  "crawlerConfiguration": {
                    "crawlerLimits": {
                      "rateLimit": 300,
                      "maxCrawlDepth": 2,
                      "maxLinksPerPage": 100,
                      "maxPagesPerUrl": 50
                    },
                    "exclusionFilters": [
                      "*/wp-admin/*", 
                      "*/login/*", 
                      "*/admin/*",
                      "*/wp-content/uploads/*"
                    ],
                    "inclusionFilters": [
                      "*/for-donors/*", 
                      "*/news/*", 
                      "*/newsroom/*",
                      "*/one-pagers-faqs/*"
                    ]
                  }
                }
              }' \
              --vector-ingestion-configuration '{
                "chunkingConfiguration": {
                  "chunkingStrategy": "FIXED_SIZE",
                  "fixedSizeChunkingConfiguration": {
                    "maxTokens": 300,
                    "overlapPercentage": 20
                  }
                }
              }' \
              --region "$CDK_DEFAULT_REGION" \
              --query 'dataSource.dataSourceId' --output text)
            
            if [ -z "$WEB_DS_ID" ] || [ "$WEB_DS_ID" = "None" ]; then
              echo "‚úó Failed to create Web Crawler Data Source"
              exit 1
            fi
            
            echo "‚úì Web Crawler Data Source created: $WEB_DS_ID"
          fi
          
          # Set primary data source ID for backward compatibility
          DS_ID="$S3_DS_ID"
          
          # Step 7: Upload initial documents
          echo "=== PHASE 5: Document Upload ==="
          
          # Upload PDF files to pdfs/ prefix for S3 data source
          if [ -d "data-sources" ]; then
            echo "Uploading PDF files to S3..."
            aws s3 cp data-sources/ s3://$DOCUMENTS_BUCKET/pdfs/ --recursive --include "*.pdf"
            
            # Upload other document files to documents/ prefix (for backward compatibility)
            aws s3 cp data-sources/ s3://$DOCUMENTS_BUCKET/documents/ --recursive --exclude "*.md" --exclude "*.pdf"
          fi
          
          echo "‚úì Initial documents uploaded"
          
          # Step 8: Update Lambda environment variables
          echo "=== PHASE 6: Lambda Configuration ==="
          
          # Update chat Lambda
          CHAT_ENV=$(aws lambda get-function-configuration \
            --function-name "$CHAT_LAMBDA_NAME" \
            --region "$CDK_DEFAULT_REGION" \
            --query 'Environment.Variables' \
            --output json)
          
          echo "$CHAT_ENV" | jq \
            --arg kb_id "$KB_ID" \
            '{Variables: (. + {KNOWLEDGE_BASE_ID: $kb_id})}' > /tmp/chat_env.json
          
          aws lambda update-function-configuration \
            --function-name "$CHAT_LAMBDA_NAME" \
            --environment file:///tmp/chat_env.json \
            --region "$CDK_DEFAULT_REGION" >/dev/null
          
          # Update data ingestion Lambda
          DATA_ENV=$(aws lambda get-function-configuration \
            --function-name "$DATA_INGESTION_LAMBDA_NAME" \
            --region "$CDK_DEFAULT_REGION" \
            --query 'Environment.Variables' \
            --output json)
          
          echo "$DATA_ENV" | jq \
            --arg kb_id "$KB_ID" \
            --arg s3_ds_id "$S3_DS_ID" \
            --arg web_ds_id "$WEB_DS_ID" \
            '{Variables: (. + {KNOWLEDGE_BASE_ID: $kb_id, S3_DATA_SOURCE_ID: $s3_ds_id, WEB_DATA_SOURCE_ID: $web_ds_id, DATA_SOURCE_ID: $s3_ds_id})}' > /tmp/data_env.json
          
          aws lambda update-function-configuration \
            --function-name "$DATA_INGESTION_LAMBDA_NAME" \
            --environment file:///tmp/data_env.json \
            --region "$CDK_DEFAULT_REGION" >/dev/null
          
          echo "‚úì Lambda environment variables updated"
          
          # Step 9: Start initial ingestion jobs for both data sources
          echo "=== PHASE 7: Knowledge Base Ingestion ==="
          
          # Start S3 ingestion job for PDFs
          S3_INGESTION_JOB_ID=$(aws bedrock-agent start-ingestion-job \
            --knowledge-base-id "$KB_ID" \
            --data-source-id "$S3_DS_ID" \
            --description "Initial ingestion of PDF documents" \
            --region "$CDK_DEFAULT_REGION" \
            --query 'ingestionJob.ingestionJobId' --output text)
          
          echo "‚úì S3 ingestion job started: $S3_INGESTION_JOB_ID"
          
          # Start Web Crawler ingestion job
          WEB_INGESTION_JOB_ID=$(aws bedrock-agent start-ingestion-job \
            --knowledge-base-id "$KB_ID" \
            --data-source-id "$WEB_DS_ID" \
            --description "Initial crawling of America's Blood Centers websites" \
            --region "$CDK_DEFAULT_REGION" \
            --query 'ingestionJob.ingestionJobId' --output text)
          
          echo "‚úì Web Crawler ingestion job started: $WEB_INGESTION_JOB_ID"
          
          # Set primary ingestion job ID for backward compatibility
          INGESTION_JOB_ID="$S3_INGESTION_JOB_ID"
          
          echo ""
          echo "========================================="
          echo "‚úÖ Deployment Complete!"
          echo "========================================="
          echo "üß† Knowledge Base ID: $KB_ID"
          echo "üìÑ S3 Data Source ID: $S3_DS_ID (PDFs with Bedrock Data Automation)"
          echo "üåê Web Crawler Data Source ID: $WEB_DS_ID (Websites)"
          echo "üìÅ Documents Bucket: $DOCUMENTS_BUCKET"
          echo "üîç Collection ARN: $COLLECTION_ARN"
          echo "üìä S3 Ingestion Job ID: $S3_INGESTION_JOB_ID"
          echo "üï∑Ô∏è Web Ingestion Job ID: $WEB_INGESTION_JOB_ID"
          echo "üåê API Gateway URL: $API_URL"
          echo "========================================="
          echo ""
          echo "========================================="
          echo "üìã Deployment Summary"
          echo "========================================="
          echo "üìÅ Documents Bucket: $DOCUMENTS_BUCKET"
          echo "üß† Knowledge Base ID: $KB_ID"
          echo "üìÑ S3 Data Source ID: $S3_DS_ID (PDFs)"
          echo "üåê Web Data Source ID: $WEB_DS_ID (Websites)"
          echo "üîó API Gateway URL: $API_URL"
          echo ""
          echo "========================================="
          echo "üöÄ Next Steps"
          echo "========================================="
          echo "1Ô∏è‚É£ Wait 5-10 minutes for ingestion jobs to complete"
          echo ""
          echo "2Ô∏è‚É£ Test the API:"
          echo "   curl -X POST $API_URL \\"
          echo "     -H 'Content-Type: application/json' \\"
          echo "     -d '{\"message\":\"How many people donate blood?\",\"language\":\"en\"}'"
          echo ""
          echo "3Ô∏è‚É£ Test Spanish support:"
          echo "   curl -X POST $API_URL \\"
          echo "     -H 'Content-Type: application/json' \\"
          echo "     -d '{\"message\":\"¬øCu√°ntas personas donan sangre?\",\"language\":\"es\"}'"
          echo ""
          echo "4Ô∏è‚É£ Monitor ingestion jobs:"
          echo "   # S3 Data Source (PDFs):"
          echo "   aws bedrock-agent get-ingestion-job \\"
          echo "     --knowledge-base-id $KB_ID \\"
          echo "     --data-source-id $S3_DS_ID \\"
          echo "     --ingestion-job-id $S3_INGESTION_JOB_ID \\"
          echo "     --region $CDK_DEFAULT_REGION"
          echo ""
          echo "   # Web Crawler Data Source:"
          echo "   aws bedrock-agent get-ingestion-job \\"
          echo "     --knowledge-base-id $KB_ID \\"
          echo "     --data-source-id $WEB_DS_ID \\"
          echo "     --ingestion-job-id $WEB_INGESTION_JOB_ID \\"
          echo "     --region $CDK_DEFAULT_REGION"
          echo ""
          echo "5Ô∏è‚É£ Add more documents:"
          echo "   # Add PDFs (uses Bedrock Data Automation parser):"
          echo "   aws s3 cp document.pdf s3://$DOCUMENTS_BUCKET/pdfs/"
          echo "   aws bedrock-agent start-ingestion-job \\"
          echo "     --knowledge-base-id $KB_ID \\"
          echo "     --data-source-id $S3_DS_ID \\"
          echo "     --region $CDK_DEFAULT_REGION"
          echo ""
          echo "   # Re-crawl websites (automatic parsing):"
          echo "   aws bedrock-agent start-ingestion-job \\"
          echo "     --knowledge-base-id $KB_ID \\"
          echo "     --data-source-id $WEB_DS_ID \\"
          echo "     --region $CDK_DEFAULT_REGION"
          echo ""
          echo "üìä Monitor resources:"
          echo "  ‚Ä¢ Knowledge Base: https://console.aws.amazon.com/bedrock/home?region=${CDK_DEFAULT_REGION}#/knowledge-bases"
          echo "  ‚Ä¢ OpenSearch: https://console.aws.amazon.com/aos/home?region=${CDK_DEFAULT_REGION}#opensearch/collections"
          echo "  ‚Ä¢ S3 Bucket: https://s3.console.aws.amazon.com/s3/buckets/$DOCUMENTS_BUCKET"
          echo ""
          echo "üí∞ Estimated monthly cost: \$8-20 (vs \$20+ for Q Business)"
          echo "üéØ Features: Dual data sources, bilingual support, automatic web crawling"
          echo ""
        fi

  post_build:
    commands:
      - echo "========================================="
      - echo "üéâ Deployment Complete"
      - echo "========================================="
      - |
        if [ "$ACTION" = "deploy" ]; then
          echo "‚úÖ Bedrock chatbot deployed successfully"
          echo "üìä Check CloudWatch Logs for Lambda execution details"
          echo "üß† Test Knowledge Base retrieval via the chat API"
          echo "üåê Monitor ingestion jobs in Bedrock console"
        else
          echo "‚úÖ Stack destroyed successfully"
        fi

artifacts:
  files:
    - "**/*"
  base-directory: "Backend/cdk.out"