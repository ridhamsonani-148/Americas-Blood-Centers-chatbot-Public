version: 0.2

phases:
  install:
    runtime-versions:
      nodejs: 20
      python: 3.11
    commands:
      - echo "Installing AWS CDK CLI..."
      - npm install -g aws-cdk@latest
      - cd Backend
      - npm install
      - echo "Checking AWS permissions..."
      - aws sts get-caller-identity
      - echo "Checking Bedrock model availability..."
      - aws bedrock list-foundation-models --region ${CDK_DEFAULT_REGION:-us-east-1} --query "modelSummaries[?contains(modelId, 'claude-3-haiku') || contains(modelId, 'titan-embed')].{ModelId:modelId,Status:modelLifecycle.status}" --output table || echo "Warning: Could not list Bedrock models"

  pre_build:
    commands:
      - echo "Building TypeScript..."
      - npm run build
      - echo "Installing Lambda dependencies..."
      - cd lambda && pip install -r requirements.txt -t . && cd ..
      - echo "Bootstrapping CDK..."
      - cdk bootstrap --require-approval never
      - echo "Setting default environment variables..."
      - export PROJECT_NAME=${PROJECT_NAME:-americas-blood-centers-bedrock}
      - export MODEL_ID=${MODEL_ID:-anthropic.claude-3-haiku-20240307-v1:0}
      - export EMBEDDING_MODEL_ID=${EMBEDDING_MODEL_ID:-amazon.titan-embed-text-v1}
      - export CDK_DEFAULT_REGION=${CDK_DEFAULT_REGION:-us-east-1}
      - echo "Configuration - Project: $PROJECT_NAME, Region: $CDK_DEFAULT_REGION, Model: $MODEL_ID"

  build:
    commands:
      - |
        if [ "$ACTION" = "destroy" ]; then
          echo "Destroying stack..."
          cdk destroy AmericasBloodCentersBedrockStack --force \
            --context projectName="$PROJECT_NAME" \
            --context modelId="$MODEL_ID" \
            --context embeddingModelId="$EMBEDDING_MODEL_ID"
        else
          echo "========================================="
          echo "Deploying America's Blood Centers Bedrock Chatbot"
          echo "========================================="
          
          echo "Step 1: Deploying CDK stack..."
          cdk deploy AmericasBloodCentersBedrockStack --require-approval never \
            --context projectName="$PROJECT_NAME" \
            --context modelId="$MODEL_ID" \
            --context embeddingModelId="$EMBEDDING_MODEL_ID" \
            --outputs-file outputs.json
          
          if [ $? -ne 0 ]; then
            echo "âœ— CDK deployment failed"
            exit 1
          fi
          
          echo "âœ“ CDK deployment successful"
          
          echo "Step 2: Extracting outputs..."
          DOCUMENTS_BUCKET=$(cat outputs.json | jq -r '.AmericasBloodCentersBedrockStack.DocumentsBucketName // empty')
          KB_ROLE_ARN=$(cat outputs.json | jq -r '.AmericasBloodCentersBedrockStack.KnowledgeBaseRoleArn // empty')
          CHAT_LAMBDA_NAME=$(cat outputs.json | jq -r '.AmericasBloodCentersBedrockStack.ChatLambdaFunctionName // empty')
          DATA_INGESTION_LAMBDA_NAME=$(cat outputs.json | jq -r '.AmericasBloodCentersBedrockStack.DataIngestionFunctionName // empty')
          AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
          
          if [ -z "$DOCUMENTS_BUCKET" ] || [ -z "$KB_ROLE_ARN" ] || [ -z "$CHAT_LAMBDA_NAME" ]; then
            echo "âœ— Failed to extract required outputs from CDK deployment"
            echo "Documents Bucket: $DOCUMENTS_BUCKET"
            echo "KB Role ARN: $KB_ROLE_ARN"
            echo "Chat Lambda: $CHAT_LAMBDA_NAME"
            exit 1
          fi
          
          echo "âœ“ Outputs extracted successfully"
          echo "  Documents Bucket: $DOCUMENTS_BUCKET"
          echo "  KB Role ARN: $KB_ROLE_ARN"
          echo "  Chat Lambda: $CHAT_LAMBDA_NAME"
          echo "  Data Ingestion Lambda: $DATA_INGESTION_LAMBDA_NAME"
          
          echo ""
          echo "========================================="
          echo "Creating Knowledge Base Resources"
          echo "========================================="
          
          # Step 3: Create OpenSearch Serverless Collection
          echo "Step 3: Creating OpenSearch Serverless collection..."
          COLLECTION_NAME="${PROJECT_NAME}-vectors"
          
          # Check if collection already exists
          COLLECTION_ARN=$(aws opensearchserverless list-collections \
            --region "$CDK_DEFAULT_REGION" \
            --query "collectionSummaries[?name=='$COLLECTION_NAME'].arn | [0]" \
            --output text 2>/dev/null || echo "")
          
          if [ -n "$COLLECTION_ARN" ] && [ "$COLLECTION_ARN" != "None" ] && [ "$COLLECTION_ARN" != "null" ]; then
            echo "âœ“ Found existing collection: $COLLECTION_ARN"
          else
            echo "Creating OpenSearch Serverless policies..."
            
            # Create network security policy (ignore errors if exists)
            echo "Creating network security policy..."
            aws opensearchserverless create-security-policy \
              --name "${PROJECT_NAME}-network-policy" \
              --type network \
              --policy '[{"Rules":[{"Resource":["collection/'$COLLECTION_NAME'"],"ResourceType":"collection"}],"AllowFromPublic":true}]' \
              --region "$CDK_DEFAULT_REGION" 2>/dev/null || echo "Network policy may already exist"
            
            # Create encryption security policy (ignore errors if exists)
            echo "Creating encryption security policy..."
            aws opensearchserverless create-security-policy \
              --name "${PROJECT_NAME}-encryption-policy" \
              --type encryption \
              --policy '{"Rules":[{"Resource":["collection/'$COLLECTION_NAME'"],"ResourceType":"collection"}],"AWSOwnedKey":true}' \
              --region "$CDK_DEFAULT_REGION" 2>/dev/null || echo "Encryption policy may already exist"
            
            # Create data access policy (ignore errors if exists)
            echo "Creating data access policy..."
            aws opensearchserverless create-access-policy \
              --name "${PROJECT_NAME}-data-access-policy" \
              --type data \
              --policy '[{"Rules":[{"Resource":["collection/'$COLLECTION_NAME'"],"Permission":["aoss:CreateCollectionItems","aoss:DeleteCollectionItems","aoss:UpdateCollectionItems","aoss:DescribeCollectionItems"],"ResourceType":"collection"},{"Resource":["index/'$COLLECTION_NAME'/*"],"Permission":["aoss:CreateIndex","aoss:DeleteIndex","aoss:UpdateIndex","aoss:DescribeIndex","aoss:ReadDocument","aoss:WriteDocument"],"ResourceType":"index"}],"Principal":["'$AWS_ACCOUNT_ID'"]}]' \
              --region "$CDK_DEFAULT_REGION" 2>/dev/null || echo "Data access policy may already exist"
            
            # Wait for policies to propagate
            echo "Waiting for policies to propagate..."
            sleep 15
            
            # Create collection
            echo "Creating OpenSearch Serverless collection..."
            COLLECTION_CREATE_OUTPUT=$(aws opensearchserverless create-collection \
              --name "$COLLECTION_NAME" \
              --type VECTORSEARCH \
              --description "Vector collection for America's Blood Centers knowledge base" \
              --region "$CDK_DEFAULT_REGION" 2>&1)
            
            if [ $? -eq 0 ]; then
              COLLECTION_ARN=$(echo "$COLLECTION_CREATE_OUTPUT" | jq -r '.createCollectionDetail.arn // empty')
              echo "âœ“ Collection creation initiated: $COLLECTION_ARN"
            else
              echo "âœ— Failed to create OpenSearch collection"
              echo "Error: $COLLECTION_CREATE_OUTPUT"
              exit 1
            fi
          fi
          
          # Step 4: Wait for collection to be active
          echo "Step 4: Waiting for collection to be active..."
          MAX_WAIT=900  # Increased to 15 minutes
          ELAPSED=0
          while [ $ELAPSED -lt $MAX_WAIT ]; do
            STATUS=$(aws opensearchserverless list-collections \
              --region "$CDK_DEFAULT_REGION" \
              --query "collectionSummaries[?name=='$COLLECTION_NAME'].status | [0]" \
              --output text 2>/dev/null || echo "ERROR")
            
            echo "  Collection status: $STATUS (waited ${ELAPSED}s/${MAX_WAIT}s)"
            
            if [ "$STATUS" = "ACTIVE" ]; then
              echo "âœ“ Collection is active"
              # Get the final collection ARN
              COLLECTION_ARN=$(aws opensearchserverless list-collections \
                --region "$CDK_DEFAULT_REGION" \
                --query "collectionSummaries[?name=='$COLLECTION_NAME'].arn | [0]" \
                --output text)
              echo "  Final Collection ARN: $COLLECTION_ARN"
              break
            elif [ "$STATUS" = "FAILED" ]; then
              echo "âœ— Collection creation failed"
              # Get collection details for debugging
              aws opensearchserverless list-collections \
                --region "$CDK_DEFAULT_REGION" \
                --query "collectionSummaries[?name=='$COLLECTION_NAME']" \
                --output json
              exit 1
            elif [ "$STATUS" = "ERROR" ] || [ -z "$STATUS" ]; then
              echo "âœ— Failed to get collection status"
              exit 1
            fi
            
            sleep 30
            ELAPSED=$((ELAPSED + 30))
          done
          
          if [ $ELAPSED -ge $MAX_WAIT ]; then
            echo "âœ— Timeout waiting for collection to be active after ${MAX_WAIT} seconds"
            echo "Current status: $STATUS"
            exit 1
          fi
          
          # Step 5: Create Knowledge Base
          echo "Step 5: Creating Bedrock Knowledge Base..."
          KB_ID=$(aws bedrock-agent list-knowledge-bases \
            --region "$CDK_DEFAULT_REGION" \
            --query "knowledgeBaseSummaries[?name=='${PROJECT_NAME}-knowledge-base'].knowledgeBaseId | [0]" \
            --output text 2>/dev/null || echo "")
          
          if [ -n "$KB_ID" ] && [ "$KB_ID" != "None" ] && [ "$KB_ID" != "null" ]; then
            echo "âœ“ Found existing Knowledge Base: $KB_ID"
          else
            echo "Creating new Bedrock Knowledge Base..."
            KB_CREATE_OUTPUT=$(aws bedrock-agent create-knowledge-base \
              --name "${PROJECT_NAME}-knowledge-base" \
              --description "Knowledge base for America's Blood Centers chatbot" \
              --role-arn "$KB_ROLE_ARN" \
              --knowledge-base-configuration '{
                "type": "VECTOR",
                "vectorKnowledgeBaseConfiguration": {
                  "embeddingModelArn": "arn:aws:bedrock:'"$CDK_DEFAULT_REGION"'::foundation-model/'"$EMBEDDING_MODEL_ID"'"
                }
              }' \
              --storage-configuration '{
                "type": "OPENSEARCH_SERVERLESS",
                "opensearchServerlessConfiguration": {
                  "collectionArn": "'"$COLLECTION_ARN"'",
                  "vectorIndexName": "bedrock-knowledge-base-default-index",
                  "fieldMapping": {
                    "vectorField": "bedrock-knowledge-base-default-vector",
                    "textField": "AMAZON_BEDROCK_TEXT_CHUNK",
                    "metadataField": "AMAZON_BEDROCK_METADATA"
                  }
                }
              }' \
              --region "$CDK_DEFAULT_REGION" 2>&1)
            
            if [ $? -eq 0 ]; then
              KB_ID=$(echo "$KB_CREATE_OUTPUT" | jq -r '.knowledgeBase.knowledgeBaseId // empty')
              echo "âœ“ Knowledge Base created: $KB_ID"
            else
              echo "âœ— Failed to create Knowledge Base"
              echo "Error: $KB_CREATE_OUTPUT"
              echo "Collection ARN: $COLLECTION_ARN"
              echo "KB Role ARN: $KB_ROLE_ARN"
              exit 1
            fi
          fi
          
          # Step 4: Create Data Sources (S3 for PDFs + Web Crawler for Websites)
          echo "Step 4: Creating Data Sources..."
          BUCKET_ARN="arn:aws:s3:::${DOCUMENTS_BUCKET}"
          
          # Data Source 1: S3 for PDFs with Bedrock Data Automation Parser
          echo "Creating S3 Data Source for PDFs..."
          S3_DS_ID=$(aws bedrock-agent list-data-sources \
            --knowledge-base-id "$KB_ID" \
            --region "$CDK_DEFAULT_REGION" \
            --query "dataSourceSummaries[?name=='blood-centers-pdfs'].dataSourceId | [0]" \
            --output text 2>/dev/null || echo "")
          
          if [ -n "$S3_DS_ID" ] && [ "$S3_DS_ID" != "None" ] && [ "$S3_DS_ID" != "null" ]; then
            echo "âœ“ Found existing S3 Data Source: $S3_DS_ID"
          else
            echo "Creating new S3 Data Source for PDFs..."
            S3_DS_ID=$(aws bedrock-agent create-data-source \
              --name "blood-centers-pdfs" \
              --description "PDF documents about blood donation with Bedrock Data Automation parser" \
              --knowledge-base-id "$KB_ID" \
              --data-source-configuration '{
                "type": "S3",
                "s3Configuration": {
                  "bucketArn": "'"$BUCKET_ARN"'",
                  "inclusionPrefixes": ["pdfs/"]
                }
              }' \
              --vector-ingestion-configuration '{
                "chunkingConfiguration": {
                  "chunkingStrategy": "FIXED_SIZE",
                  "fixedSizeChunkingConfiguration": {
                    "maxTokens": 300,
                    "overlapPercentage": 20
                  }
                },
                "parsingConfiguration": {
                  "parsingStrategy": "BEDROCK_DATA_AUTOMATION",
                  "bedrockDataAutomationConfiguration": {
                    "parsingPrompt": "Extract all text content from PDF documents, preserving structure and formatting. Focus on blood donation information, statistics, guidelines, and eligibility criteria."
                  }
                }
              }' \
              --region "$CDK_DEFAULT_REGION" \
              --query 'dataSource.dataSourceId' --output text)
            
            if [ -z "$S3_DS_ID" ] || [ "$S3_DS_ID" = "None" ]; then
              echo "âœ— Failed to create S3 Data Source"
              exit 1
            fi
            
            echo "âœ“ S3 Data Source created: $S3_DS_ID"
          fi
          
          # Data Source 2: Web Crawler for Websites
          echo "Creating Web Crawler Data Source..."
          WEB_DS_ID=$(aws bedrock-agent list-data-sources \
            --knowledge-base-id "$KB_ID" \
            --region "$CDK_DEFAULT_REGION" \
            --query "dataSourceSummaries[?name=='blood-centers-websites'].dataSourceId | [0]" \
            --output text 2>/dev/null || echo "")
          
          if [ -n "$WEB_DS_ID" ] && [ "$WEB_DS_ID" != "None" ] && [ "$WEB_DS_ID" != "null" ]; then
            echo "âœ“ Found existing Web Crawler Data Source: $WEB_DS_ID"
          else
            echo "Creating new Web Crawler Data Source..."
            WEB_DS_ID=$(aws bedrock-agent create-data-source \
              --name "blood-centers-websites" \
              --description "Blood donation websites and news from America's Blood Centers" \
              --knowledge-base-id "$KB_ID" \
              --data-source-configuration '{
                "type": "WEB",
                "webConfiguration": {
                  "urlConfiguration": {
                    "seedUrls": [
                      {"url": "https://americasblood.org/for-donors/americas-blood-supply/"},
                      {"url": "https://americasblood.org/for-donors/find-a-blood-center/"},
                      {"url": "https://americasblood.org/news/"},
                      {"url": "https://americasblood.org/newsroom/"},
                      {"url": "https://americasblood.org/one-pagers-faqs/"}
                    ]
                  },
                  "crawlerConfiguration": {
                    "crawlerLimits": {
                      "rateLimit": 300,
                      "maxCrawlDepth": 2,
                      "maxLinksPerPage": 100,
                      "maxPagesPerUrl": 50
                    },
                    "exclusionFilters": [
                      "*/wp-admin/*", 
                      "*/login/*", 
                      "*/admin/*",
                      "*/wp-content/uploads/*"
                    ],
                    "inclusionFilters": [
                      "*/for-donors/*", 
                      "*/news/*", 
                      "*/newsroom/*",
                      "*/one-pagers-faqs/*"
                    ]
                  }
                }
              }' \
              --vector-ingestion-configuration '{
                "chunkingConfiguration": {
                  "chunkingStrategy": "FIXED_SIZE",
                  "fixedSizeChunkingConfiguration": {
                    "maxTokens": 300,
                    "overlapPercentage": 20
                  }
                }
              }' \
              --region "$CDK_DEFAULT_REGION" \
              --query 'dataSource.dataSourceId' --output text)
            
            if [ -z "$WEB_DS_ID" ] || [ "$WEB_DS_ID" = "None" ]; then
              echo "âœ— Failed to create Web Crawler Data Source"
              exit 1
            fi
            
            echo "âœ“ Web Crawler Data Source created: $WEB_DS_ID"
          fi
          
          # Set primary data source ID for backward compatibility
          DS_ID="$S3_DS_ID"
          
          # Step 5: Upload initial documents
          echo "Step 5: Uploading initial documents..."
          
          # Upload PDF files to pdfs/ prefix for S3 data source
          if [ -d "data-sources" ]; then
            echo "Uploading PDF files to S3..."
            aws s3 cp data-sources/ s3://$DOCUMENTS_BUCKET/pdfs/ --recursive --include "*.pdf"
            
            # Upload other document files to documents/ prefix (for backward compatibility)
            aws s3 cp data-sources/ s3://$DOCUMENTS_BUCKET/documents/ --recursive --exclude "*.md" --exclude "*.pdf"
          fi
          
          echo "âœ“ Initial documents uploaded"
          
          # Step 6: Update Lambda environment variables
          echo "Step 6: Updating Lambda environment variables..."
          
          # Update chat Lambda
          CHAT_ENV=$(aws lambda get-function-configuration \
            --function-name "$CHAT_LAMBDA_NAME" \
            --region "$CDK_DEFAULT_REGION" \
            --query 'Environment.Variables' \
            --output json)
          
          echo "$CHAT_ENV" | jq \
            --arg kb_id "$KB_ID" \
            '{Variables: (. + {KNOWLEDGE_BASE_ID: $kb_id})}' > /tmp/chat_env.json
          
          aws lambda update-function-configuration \
            --function-name "$CHAT_LAMBDA_NAME" \
            --environment file:///tmp/chat_env.json \
            --region "$CDK_DEFAULT_REGION" >/dev/null
          
          # Update data ingestion Lambda
          DATA_ENV=$(aws lambda get-function-configuration \
            --function-name "$DATA_INGESTION_LAMBDA_NAME" \
            --region "$CDK_DEFAULT_REGION" \
            --query 'Environment.Variables' \
            --output json)
          
          echo "$DATA_ENV" | jq \
            --arg kb_id "$KB_ID" \
            --arg s3_ds_id "$S3_DS_ID" \
            --arg web_ds_id "$WEB_DS_ID" \
            '{Variables: (. + {KNOWLEDGE_BASE_ID: $kb_id, S3_DATA_SOURCE_ID: $s3_ds_id, WEB_DATA_SOURCE_ID: $web_ds_id, DATA_SOURCE_ID: $s3_ds_id})}' > /tmp/data_env.json
          
          aws lambda update-function-configuration \
            --function-name "$DATA_INGESTION_LAMBDA_NAME" \
            --environment file:///tmp/data_env.json \
            --region "$CDK_DEFAULT_REGION" >/dev/null
          
          echo "âœ“ Lambda environment variables updated"
          
          # Step 7: Start initial ingestion jobs for both data sources
          echo "Step 7: Starting Knowledge Base ingestion jobs..."
          
          # Start S3 ingestion job for PDFs
          S3_INGESTION_JOB_ID=$(aws bedrock-agent start-ingestion-job \
            --knowledge-base-id "$KB_ID" \
            --data-source-id "$S3_DS_ID" \
            --description "Initial ingestion of PDF documents" \
            --region "$CDK_DEFAULT_REGION" \
            --query 'ingestionJob.ingestionJobId' --output text)
          
          echo "âœ“ S3 ingestion job started: $S3_INGESTION_JOB_ID"
          
          # Start Web Crawler ingestion job
          WEB_INGESTION_JOB_ID=$(aws bedrock-agent start-ingestion-job \
            --knowledge-base-id "$KB_ID" \
            --data-source-id "$WEB_DS_ID" \
            --description "Initial crawling of America's Blood Centers websites" \
            --region "$CDK_DEFAULT_REGION" \
            --query 'ingestionJob.ingestionJobId' --output text)
          
          echo "âœ“ Web Crawler ingestion job started: $WEB_INGESTION_JOB_ID"
          
          # Set primary ingestion job ID for backward compatibility
          INGESTION_JOB_ID="$S3_INGESTION_JOB_ID"
          
          echo ""
          echo "========================================="
          echo "âœ“ Deployment Complete!"
          echo "========================================="
          echo "Knowledge Base ID: $KB_ID"
          echo "S3 Data Source ID: $S3_DS_ID (PDFs with Bedrock Data Automation)"
          echo "Web Crawler Data Source ID: $WEB_DS_ID (Websites)"
          echo "Documents Bucket: $DOCUMENTS_BUCKET"
          echo "Collection ARN: $COLLECTION_ARN"
          echo "S3 Ingestion Job ID: $S3_INGESTION_JOB_ID"
          echo "Web Ingestion Job ID: $WEB_INGESTION_JOB_ID"
          echo "========================================="
          
          # Extract API URL
          API_URL=$(cat outputs.json | jq -r '.AmericasBloodCentersBedrockStack.ApiGatewayUrl // empty')
          
          echo ""
          echo "========================================="
          echo "Deployment Summary"
          echo "========================================="
          echo "Documents Bucket: $DOCUMENTS_BUCKET"
          echo "Knowledge Base ID: $KB_ID"
          echo "S3 Data Source ID: $S3_DS_ID (PDFs)"
          echo "Web Data Source ID: $WEB_DS_ID (Websites)"
          echo "API Gateway URL: $API_URL"
          echo ""
          echo "========================================="
          echo "Next Steps"
          echo "========================================="
          echo "1. Wait 5-10 minutes for ingestion job to complete"
          echo ""
          echo "2. Test the API:"
          echo "   curl -X POST $API_URL \\"
          echo "     -H 'Content-Type: application/json' \\"
          echo "     -d '{\"message\":\"How many people donate blood?\",\"language\":\"en\"}'"
          echo ""
          echo "3. Test Spanish support:"
          echo "   curl -X POST $API_URL \\"
          echo "     -H 'Content-Type: application/json' \\"
          echo "     -d '{\"message\":\"Â¿CuÃ¡ntas personas donan sangre?\",\"language\":\"es\"}'"
          echo ""
          echo "4. Monitor ingestion jobs:"
          echo "   # S3 Data Source (PDFs):"
          echo "   aws bedrock-agent get-ingestion-job \\"
          echo "     --knowledge-base-id $KB_ID \\"
          echo "     --data-source-id $S3_DS_ID \\"
          echo "     --ingestion-job-id $S3_INGESTION_JOB_ID"
          echo ""
          echo "   # Web Crawler Data Source:"
          echo "   aws bedrock-agent get-ingestion-job \\"
          echo "     --knowledge-base-id $KB_ID \\"
          echo "     --data-source-id $WEB_DS_ID \\"
          echo "     --ingestion-job-id $WEB_INGESTION_JOB_ID"
          echo ""
          echo "5. Add more documents:"
          echo "   # Add PDFs (uses Bedrock Data Automation parser):"
          echo "   aws s3 cp document.pdf s3://$DOCUMENTS_BUCKET/pdfs/"
          echo "   aws bedrock-agent start-ingestion-job \\"
          echo "     --knowledge-base-id $KB_ID \\"
          echo "     --data-source-id $S3_DS_ID"
          echo ""
          echo "   # Re-crawl websites (automatic parsing):"
          echo "   aws bedrock-agent start-ingestion-job \\"
          echo "     --knowledge-base-id $KB_ID \\"
          echo "     --data-source-id $WEB_DS_ID"
          echo ""
          echo "ðŸ“Š Monitor resources:"
          echo "- Knowledge Base: https://console.aws.amazon.com/bedrock/home?region=${CDK_DEFAULT_REGION}#/knowledge-bases"
          echo "- OpenSearch: https://console.aws.amazon.com/aos/home?region=${CDK_DEFAULT_REGION}#opensearch/collections"
          echo "- S3 Bucket: https://s3.console.aws.amazon.com/s3/buckets/$DOCUMENTS_BUCKET"
          echo ""
          echo "ðŸ’° Estimated monthly cost: \$8-20 (vs \$20+ for Q Business)"
          echo ""
        fi

  post_build:
    commands:
      - echo "========================================="
      - echo "Deployment Complete"
      - echo "========================================="
      - |
        if [ "$ACTION" = "deploy" ]; then
          echo "âœ… Bedrock chatbot deployed successfully"
          echo "ðŸ“Š Check CloudWatch Logs for Lambda execution details"
          echo "ðŸ§  Test Knowledge Base retrieval via the chat API"
        else
          echo "âœ… Stack destroyed successfully"
        fi

artifacts:
  files:
    - "**/*"
  base-directory: "Backend/cdk.out"