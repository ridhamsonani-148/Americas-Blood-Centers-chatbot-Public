version: 0.2

phases:
  install:
    runtime-versions:
      nodejs: 20
      python: 3.11
    commands:
      - echo "Installing AWS CDK CLI..."
      - npm install -g aws-cdk@latest
      - cd Backend
      - npm install

  pre_build:
    commands:
      - echo "=== Building TypeScript ==="
      - npm run build
      - echo "‚úÖ TypeScript build completed"
      - echo "=== Installing Lambda dependencies ==="
      - cd lambda && pip install -r requirements.txt -t . && cd ..
      - echo "‚úÖ Lambda dependencies installed"
      - echo "=== Bootstrapping CDK Environment ==="
      - |
        # Set default PROJECT_NAME for bootstrap if not provided
        if [ -z "$PROJECT_NAME" ]; then
          PROJECT_NAME="abc"
        fi
        cdk bootstrap --require-approval never \
          --context projectName="$PROJECT_NAME" \
          --context modelId="${MODEL_ID:-anthropic.claude-3-haiku-20240307-v1:0}" \
          --context embeddingModelId="${EMBEDDING_MODEL_ID:-amazon.titan-embed-text-v1}"
      - echo "‚úÖ CDK Bootstrap completed"

  build:
    commands:
      - |
        if [ "$ACTION" = "destroy" ]; then
          echo "=== Destroying Stack ==="
          cdk destroy AmericasBloodCentersBedrockStack --force \
            --context projectName="$PROJECT_NAME" \
            --context modelId="$MODEL_ID" \
            --context embeddingModelId="$EMBEDDING_MODEL_ID"
          echo "‚úÖ Stack destroyed successfully"
        else
          echo "========================================="
          echo "Deploying America's Blood Centers Bedrock Chatbot"
          echo "========================================="
          
          echo "=== PHASE 1: CDK Infrastructure Deployment ==="
          
          # Set default PROJECT_NAME if not provided
          if [ -z "$PROJECT_NAME" ]; then
            PROJECT_NAME="abc"
            echo "üîç Using default PROJECT_NAME: $PROJECT_NAME"
          else
            echo "üîç Using provided PROJECT_NAME: $PROJECT_NAME"
          fi
          
          cdk deploy AmericasBloodCentersBedrockStack --require-approval never \
            --context projectName="$PROJECT_NAME" \
            --context modelId="$MODEL_ID" \
            --context embeddingModelId="$EMBEDDING_MODEL_ID" \
            --outputs-file outputs.json
          
          if [ $? -ne 0 ]; then
            echo "‚ùå ERROR: CDK deployment failed"
            exit 1
          fi
          
          echo "‚úÖ CDK deployment successful"
          
          echo "=== PHASE 2: Extracting CDK Outputs ==="
          STACK_NAME="AmericasBloodCentersBedrockStack"
          DOCUMENTS_BUCKET=$(cat outputs.json | jq -r '.AmericasBloodCentersBedrockStack.DocumentsBucketName // empty')
          KB_ROLE_ARN=$(cat outputs.json | jq -r '.AmericasBloodCentersBedrockStack.KnowledgeBaseRoleArn // empty')
          CHAT_LAMBDA_NAME=$(cat outputs.json | jq -r '.AmericasBloodCentersBedrockStack.ChatLambdaFunctionName // empty')
          DATA_INGESTION_LAMBDA_NAME=$(cat outputs.json | jq -r '.AmericasBloodCentersBedrockStack.DataIngestionFunctionName // empty')
          API_URL=$(cat outputs.json | jq -r '.AmericasBloodCentersBedrockStack.ApiGatewayUrl // empty')
          AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
          
          if [ -z "$DOCUMENTS_BUCKET" ] || [ -z "$KB_ROLE_ARN" ] || [ -z "$CHAT_LAMBDA_NAME" ]; then
            echo "‚ùå ERROR: Failed to extract required outputs from CDK deployment"
            echo "Documents Bucket: $DOCUMENTS_BUCKET"
            echo "KB Role ARN: $KB_ROLE_ARN"
            echo "Chat Lambda: $CHAT_LAMBDA_NAME"
            exit 1
          fi
          
          echo "‚úÖ Outputs extracted successfully"
          echo "  üìÅ Documents Bucket: $DOCUMENTS_BUCKET"
          echo "  üîê KB Role ARN: $KB_ROLE_ARN"
          echo "  üí¨ Chat Lambda: $CHAT_LAMBDA_NAME"
          echo "  üìä Data Ingestion Lambda: $DATA_INGESTION_LAMBDA_NAME"
          echo "  üåê API Gateway URL: $API_URL"
          echo ""
          echo "========================================="
          echo "=== PHASE 3: Knowledge Base Resources ==="
          echo "========================================="
          
          # Step 3: Create or reuse OpenSearch Serverless Collection
          echo "Step 3: Creating or reusing OpenSearch Serverless collection..."
          
          # Use a consistent collection name (no timestamp for reusability)
          COLLECTION_NAME="${PROJECT_NAME}-kb-collection"
          echo "üîç Debug: PROJECT_NAME='$PROJECT_NAME'"
          echo "üîç Debug: COLLECTION_NAME='$COLLECTION_NAME'"
          echo "üîç Debug: Collection name length: ${#COLLECTION_NAME}"
          
          # Check if collection already exists
          EXISTING_COLLECTION_ARN=$(aws opensearchserverless list-collections \
            --region "$CDK_DEFAULT_REGION" \
            --query "collectionSummaries[?name=='$COLLECTION_NAME'].arn | [0]" \
            --output text 2>/dev/null || echo "")
          
          if [ -n "$EXISTING_COLLECTION_ARN" ] && [ "$EXISTING_COLLECTION_ARN" != "None" ] && [ "$EXISTING_COLLECTION_ARN" != "null" ]; then
            echo "‚úì Found existing OpenSearch collection: $EXISTING_COLLECTION_ARN"
            COLLECTION_ARN="$EXISTING_COLLECTION_ARN"
            
            # Check if collection is active
            COLLECTION_STATUS=$(aws opensearchserverless list-collections \
              --region "$CDK_DEFAULT_REGION" \
              --query "collectionSummaries[?name=='$COLLECTION_NAME'].status | [0]" \
              --output text)
            
            if [ "$COLLECTION_STATUS" = "ACTIVE" ]; then
              echo "‚úì Collection is already active"
            else
              echo "‚ö† Collection exists but status is: $COLLECTION_STATUS"
              echo "Waiting for collection to become active..."
            fi
          else
            echo "Creating new OpenSearch Serverless collection..."
            
            # Create or update network security policy
            echo "Creating/updating network security policy..."
            if aws opensearchserverless get-security-policy --name "${PROJECT_NAME}-network-policy" --type network --region "$CDK_DEFAULT_REGION" >/dev/null 2>&1; then
              echo "‚úì Network security policy already exists, updating if needed..."
              # Get current policy version
              NETWORK_POLICY_VERSION=$(aws opensearchserverless get-security-policy \
                --name "${PROJECT_NAME}-network-policy" \
                --type network \
                --region "$CDK_DEFAULT_REGION" \
                --query 'securityPolicyDetail.policyVersion' \
                --output text)
              # Update the policy to include the current collection
              aws opensearchserverless update-security-policy \
                --name "${PROJECT_NAME}-network-policy" \
                --type network \
                --policy-version "$NETWORK_POLICY_VERSION" \
                --policy '[{"Rules":[{"Resource":["collection/'$COLLECTION_NAME'"],"ResourceType":"collection"}],"AllowFromPublic":true}]' \
                --region "$CDK_DEFAULT_REGION" && echo "‚úì Network security policy updated"
            else
              aws opensearchserverless create-security-policy \
                --name "${PROJECT_NAME}-network-policy" \
                --type network \
                --policy '[{"Rules":[{"Resource":["collection/'$COLLECTION_NAME'"],"ResourceType":"collection"}],"AllowFromPublic":true}]' \
                --region "$CDK_DEFAULT_REGION" && echo "‚úì Network security policy created"
            fi
            
            # Create or update encryption security policy
            echo "Creating/updating encryption security policy..."
            if aws opensearchserverless get-security-policy --name "${PROJECT_NAME}-encryption-policy" --type encryption --region "$CDK_DEFAULT_REGION" >/dev/null 2>&1; then
              echo "‚úì Encryption security policy already exists, updating if needed..."
              # Get current policy version
              ENCRYPTION_POLICY_VERSION=$(aws opensearchserverless get-security-policy \
                --name "${PROJECT_NAME}-encryption-policy" \
                --type encryption \
                --region "$CDK_DEFAULT_REGION" \
                --query 'securityPolicyDetail.policyVersion' \
                --output text)
              aws opensearchserverless update-security-policy \
                --name "${PROJECT_NAME}-encryption-policy" \
                --type encryption \
                --policy-version "$ENCRYPTION_POLICY_VERSION" \
                --policy '{"Rules":[{"Resource":["collection/'$COLLECTION_NAME'"],"ResourceType":"collection"}],"AWSOwnedKey":true}' \
                --region "$CDK_DEFAULT_REGION" && echo "‚úì Encryption security policy updated"
            else
              aws opensearchserverless create-security-policy \
                --name "${PROJECT_NAME}-encryption-policy" \
                --type encryption \
                --policy '{"Rules":[{"Resource":["collection/'$COLLECTION_NAME'"],"ResourceType":"collection"}],"AWSOwnedKey":true}' \
                --region "$CDK_DEFAULT_REGION" && echo "‚úì Encryption security policy created"
            fi
            
            # Create or update data access policy
            echo "Creating/updating data access policy..."
            # Get CodeBuild role ARN for access policy
            CODEBUILD_ROLE_ARN=$(aws sts get-caller-identity --query Arn --output text | sed 's/:user\//:role\//' | sed 's/\/[^\/]*$/\/codebuild-service-role/')
            echo "üîç Debug: CodeBuild role ARN: $CODEBUILD_ROLE_ARN"
            
            if aws opensearchserverless get-access-policy --name "${PROJECT_NAME}-data-access-policy" --type data --region "$CDK_DEFAULT_REGION" >/dev/null 2>&1; then
              echo "‚úì Data access policy already exists, will update with KB role later"
            else
              # Create new data access policy
              aws opensearchserverless create-access-policy \
                --name "${PROJECT_NAME}-data-access-policy" \
                --type data \
                --policy '[{"Rules":[{"Resource":["collection/'$COLLECTION_NAME'"],"Permission":["aoss:CreateCollectionItems","aoss:DeleteCollectionItems","aoss:UpdateCollectionItems","aoss:DescribeCollectionItems"],"ResourceType":"collection"},{"Resource":["index/'$COLLECTION_NAME'/*"],"Permission":["aoss:CreateIndex","aoss:DeleteIndex","aoss:UpdateIndex","aoss:DescribeIndex","aoss:ReadDocument","aoss:WriteDocument"],"ResourceType":"index"}],"Principal":["arn:aws:iam::'$AWS_ACCOUNT_ID':root","'$CODEBUILD_ROLE_ARN'"]}]' \
                --region "$CDK_DEFAULT_REGION" && echo "‚úì Data access policy created (will update with KB role later)"
            fi
            
            # Wait for policies to propagate
            echo "Waiting for policies to propagate..."
            sleep 15
          fi
          
          # Create collection only if it doesn't exist
          if [ -z "$EXISTING_COLLECTION_ARN" ] || [ "$EXISTING_COLLECTION_ARN" = "None" ] || [ "$EXISTING_COLLECTION_ARN" = "null" ]; then
            echo "üîç Debug: EXISTING_COLLECTION_ARN is empty/None/null, creating new collection"
            echo "üîç Debug: EXISTING_COLLECTION_ARN value: '$EXISTING_COLLECTION_ARN'"
            echo "Creating OpenSearch Serverless collection..."
            COLLECTION_CREATE_OUTPUT=$(aws opensearchserverless create-collection \
              --name "$COLLECTION_NAME" \
              --type VECTORSEARCH \
              --description "Vector collection for America's Blood Centers knowledge base" \
              --region "$CDK_DEFAULT_REGION" 2>&1)
            
            COLLECTION_CREATE_EXIT_CODE=$?
            echo "üîç Debug: Collection creation exit code: $COLLECTION_CREATE_EXIT_CODE"
            echo "üîç Debug: Collection creation output: $COLLECTION_CREATE_OUTPUT"
            
            if [ $COLLECTION_CREATE_EXIT_CODE -eq 0 ]; then
              COLLECTION_ARN=$(echo "$COLLECTION_CREATE_OUTPUT" | jq -r '.createCollectionDetail.arn // empty')
              echo "‚úì Collection creation initiated: $COLLECTION_ARN"
            else
              echo "‚úó Failed to create OpenSearch collection"
              echo "Error: $COLLECTION_CREATE_OUTPUT"
              exit 1
            fi
          else
            echo "üîç Debug: Using existing collection: $EXISTING_COLLECTION_ARN"
            COLLECTION_ARN="$EXISTING_COLLECTION_ARN"
          fi
          
          # Step 4: Wait for collection to be active
          echo "Step 4: Waiting for collection to be active..."
          MAX_WAIT=900  # Increased to 15 minutes
          ELAPSED=0
          while [ $ELAPSED -lt $MAX_WAIT ]; do
            STATUS=$(aws opensearchserverless list-collections \
              --region "$CDK_DEFAULT_REGION" \
              --query "collectionSummaries[?name=='$COLLECTION_NAME'].status | [0]" \
              --output text 2>/dev/null || echo "ERROR")
            
            echo "  Collection status: $STATUS (waited ${ELAPSED}s/${MAX_WAIT}s)"
            
            if [ "$STATUS" = "ACTIVE" ]; then
              echo "‚úì Collection is active"
              # Get the final collection ARN
              COLLECTION_ARN=$(aws opensearchserverless list-collections \
                --region "$CDK_DEFAULT_REGION" \
                --query "collectionSummaries[?name=='$COLLECTION_NAME'].arn | [0]" \
                --output text)
              echo "  Final Collection ARN: $COLLECTION_ARN"
              break
            elif [ "$STATUS" = "FAILED" ]; then
              echo "‚úó Collection creation failed"
              # Get collection details for debugging
              aws opensearchserverless list-collections \
                --region "$CDK_DEFAULT_REGION" \
                --query "collectionSummaries[?name=='$COLLECTION_NAME']" \
                --output json
              exit 1
            elif [ "$STATUS" = "ERROR" ] || [ -z "$STATUS" ] || [ "$STATUS" = "None" ] || [ "$STATUS" = "null" ]; then
              echo "‚úó Failed to get collection status or collection doesn't exist"
              echo "üîç Debug: Checking if collection was actually created..."
              aws opensearchserverless list-collections \
                --region "$CDK_DEFAULT_REGION" \
                --query "collectionSummaries[?name=='$COLLECTION_NAME']" \
                --output json
              
              # If no collection found after some time, exit
              if [ $ELAPSED -gt 300 ]; then
                echo "‚úó Collection not found after 5 minutes, something went wrong"
                exit 1
              fi
            fi
            
            sleep 30
            ELAPSED=$((ELAPSED + 30))
          done
          
          if [ $ELAPSED -ge $MAX_WAIT ]; then
            echo "‚úó Timeout waiting for collection to be active after ${MAX_WAIT} seconds"
            echo "Current status: $STATUS"
            exit 1
          fi
          
          # Step 4.5: Update data access policy with Knowledge Base role
          echo "Step 4.5: Adding Knowledge Base role to OpenSearch data access policy..."
          
          # Extract just the role name from the full ARN
          KB_ROLE_NAME=$(echo "$KB_ROLE_ARN" | sed 's/.*role\///')
          echo "üîç Debug: KB Role Name: $KB_ROLE_NAME"
          echo "üîç Debug: KB Role ARN: $KB_ROLE_ARN"
          
          # Get current policy version
          echo "Getting current data access policy version..."
          POLICY_VERSION=$(aws opensearchserverless get-access-policy \
            --name "${PROJECT_NAME}-data-access-policy" \
            --type data \
            --region "$CDK_DEFAULT_REGION" \
            --query 'accessPolicyDetail.policyVersion' \
            --output text)
          
          echo "üîç Debug: Current policy version: $POLICY_VERSION"
          
          # Get CodeBuild role ARN for the policy update
          CODEBUILD_ROLE_ARN=$(aws sts get-caller-identity --query Arn --output text | sed 's/:user\//:role\//' | sed 's/\/[^\/]*$/\/codebuild-service-role/')
          
          # Update the data access policy to include the Knowledge Base role
          echo "Updating data access policy to include Knowledge Base role..."
          aws opensearchserverless update-access-policy \
            --name "${PROJECT_NAME}-data-access-policy" \
            --type data \
            --policy-version "$POLICY_VERSION" \
            --policy '[{"Rules":[{"Resource":["collection/'$COLLECTION_NAME'"],"Permission":["aoss:CreateCollectionItems","aoss:DeleteCollectionItems","aoss:UpdateCollectionItems","aoss:DescribeCollectionItems"],"ResourceType":"collection"},{"Resource":["index/'$COLLECTION_NAME'/*"],"Permission":["aoss:CreateIndex","aoss:DeleteIndex","aoss:UpdateIndex","aoss:DescribeIndex","aoss:ReadDocument","aoss:WriteDocument"],"ResourceType":"index"}],"Principal":["arn:aws:iam::'$AWS_ACCOUNT_ID':root","'$CODEBUILD_ROLE_ARN'","'$KB_ROLE_ARN'"]}]' \
            --region "$CDK_DEFAULT_REGION" && echo "‚úì Data access policy updated with Knowledge Base role"
          
          # Wait for policy update to propagate
          echo "Waiting for policy update to propagate..."
          sleep 10
          
          # Step 4.6: Create the required vector index manually (if it doesn't exist)
          echo "Step 4.6: Creating vector index in OpenSearch collection..."
          
          # Get the collection ID from the ARN
          COLLECTION_ID=$(echo "$COLLECTION_ARN" | sed 's/.*collection\///')
          echo "üîç Debug: Collection ID extracted: $COLLECTION_ID"
          
          # Check if index already exists
          echo "Checking if vector index already exists..."
          INDEX_EXISTS=$(aws opensearchserverless list-indexes \
            --region "$CDK_DEFAULT_REGION" \
            --collection-id "$COLLECTION_ID" \
            --query 'indexDetails[?name==`bedrock-knowledge-base-default-index`].name | [0]' \
            --output text 2>/dev/null || echo "")
          
          if [ -n "$INDEX_EXISTS" ] && [ "$INDEX_EXISTS" != "None" ] && [ "$INDEX_EXISTS" != "null" ]; then
            echo "‚úì Vector index already exists: bedrock-knowledge-base-default-index"
          else
            # Create the index using AWS CLI (now that we have proper permissions)
            echo "Creating bedrock-knowledge-base-default-index..."
            echo "üîç Debug: Using collection ID: $COLLECTION_ID"
            echo "üîç Debug: Region: $CDK_DEFAULT_REGION"
            
            # Try to create the index and capture both stdout and stderr
            set +e  # Temporarily disable exit on error
            INDEX_CREATION_OUTPUT=$(aws opensearchserverless create-index \
              --region "$CDK_DEFAULT_REGION" \
              --id "$COLLECTION_ID" \
              --index-name "bedrock-knowledge-base-default-index" \
              --index-schema '{
                "settings": {
                  "index.knn": true
                },
                "mappings": {
                  "properties": {
                    "bedrock-knowledge-base-default-vector": {
                      "type": "knn_vector",
                      "dimension": 1536,
                      "method": {
                        "name": "hnsw",
                        "engine": "faiss",
                        "parameters": {
                          "ef_construction": 512,
                          "m": 16
                        }
                      }
                    },
                    "AMAZON_BEDROCK_TEXT_CHUNK": {
                      "type": "text"
                    },
                    "AMAZON_BEDROCK_METADATA": {
                      "type": "text"
                    }
                  }
                }
              }' 2>&1)
            INDEX_EXIT_CODE=$?
            set -e  # Re-enable exit on error
            
            echo "üîç Debug: Index creation exit code: $INDEX_EXIT_CODE"
            echo "üîç Debug: Index creation output: $INDEX_CREATION_OUTPUT"
            
            if echo "$INDEX_CREATION_OUTPUT" | grep -q "indexDetail"; then
              echo "‚úì Vector index created successfully"
              echo "Index details: $INDEX_CREATION_OUTPUT"
            elif echo "$INDEX_CREATION_OUTPUT" | grep -q "already exists\|ResourceAlreadyExistsException"; then
              echo "‚úì Index already exists, continuing..."
            else
              echo "‚ö† Index creation response: $INDEX_CREATION_OUTPUT"
              echo "‚ö† Index creation may have failed, but continuing..."
            fi
          fi
          
          # Wait for index to be ready
          echo "Waiting for index to be ready..."
          sleep 15
          
          echo "‚úì Collection and index are ready for Knowledge Base creation"
          
          # Step 5: Create Knowledge Base with Supplemental Data Storage
          echo "Step 5: Creating Bedrock Knowledge Base with supplemental storage..."
          
          # Get supplemental bucket from CDK stack outputs
          SUPPLEMENTAL_BUCKET=$(aws cloudformation describe-stacks \
            --stack-name "$STACK_NAME" \
            --query "Stacks[0].Outputs[?OutputKey=='SupplementalBucketName'].OutputValue | [0]" \
            --output text)
          
          if [ -z "$SUPPLEMENTAL_BUCKET" ] || [ "$SUPPLEMENTAL_BUCKET" = "None" ]; then
            echo "‚úó Supplemental bucket not found in CDK stack outputs"
            exit 1
          else
            echo "‚úì Using CDK-created supplemental bucket: $SUPPLEMENTAL_BUCKET"
          fi
          
          # Permissions already handled by CDK
          echo "‚úì Supplemental bucket permissions configured by CDK"
          
          KB_ID=$(aws bedrock-agent list-knowledge-bases \
            --region "$CDK_DEFAULT_REGION" \
            --query "knowledgeBaseSummaries[?name=='${PROJECT_NAME}-knowledge-base'].knowledgeBaseId | [0]" \
            --output text 2>/dev/null || echo "")
          
          if [ -n "$KB_ID" ] && [ "$KB_ID" != "None" ] && [ "$KB_ID" != "null" ]; then
            echo "‚úì Found existing Knowledge Base: $KB_ID"
            
            # Check if Knowledge Base has supplemental storage configured
            KB_CONFIG=$(aws bedrock-agent get-knowledge-base \
              --knowledge-base-id "$KB_ID" \
              --region "$CDK_DEFAULT_REGION" \
              --query 'knowledgeBase.knowledgeBaseConfiguration.vectorKnowledgeBaseConfiguration.supplementalDataStorageConfiguration' \
              --output json 2>/dev/null || echo "null")
            
            if [ "$KB_CONFIG" = "null" ] || [ "$KB_CONFIG" = "{}" ]; then
              echo "‚ö† Knowledge Base exists but lacks supplemental storage configuration"
              echo "‚ö† Deleting existing Knowledge Base to recreate with supplemental storage..."
              aws bedrock-agent delete-knowledge-base \
                --knowledge-base-id "$KB_ID" \
                --region "$CDK_DEFAULT_REGION"
              
              # Wait for deletion to complete
              echo "Waiting for Knowledge Base deletion..."
              sleep 30
              KB_ID=""  # Clear KB_ID to trigger recreation
            else
              echo "‚úì Knowledge Base already has supplemental storage configured"
              echo "‚úì Skipping recreation - using existing Knowledge Base: $KB_ID"
            fi
          fi
          
          # Only create if KB doesn't exist or was deleted
          if [ -z "$KB_ID" ] || [ "$KB_ID" = "None" ] || [ "$KB_ID" = "null" ]; then
            echo "Creating new Bedrock Knowledge Base with supplemental storage..."
          echo "üîç Debug: KB Role ARN: $KB_ROLE_ARN"
          echo "üîç Debug: Collection ARN: $COLLECTION_ARN"
          echo "üîç Debug: Supplemental Bucket: $SUPPLEMENTAL_BUCKET"
          echo "üîç Debug: Region: $CDK_DEFAULT_REGION"
          echo "üîç Debug: Embedding Model: $EMBEDDING_MODEL_ID"
          
          # Verify bucket access and ownership
          echo "Verifying supplemental bucket access..."
          BUCKET_OWNER=$(aws s3api get-bucket-location --bucket "$SUPPLEMENTAL_BUCKET" --query 'LocationConstraint' --output text 2>/dev/null || echo "ERROR")
          if [ "$BUCKET_OWNER" != "ERROR" ]; then
            echo "‚úì Bucket access verified"
            echo "üîç Debug: Bucket region: $BUCKET_OWNER"
          else
            echo "‚ö† Could not verify bucket access"
          fi
          
          # Test if the role can access the bucket (simulate what Bedrock will do)
          echo "Testing bucket permissions..."
          aws s3 ls "s3://$SUPPLEMENTAL_BUCKET" >/dev/null 2>&1 && echo "‚úì Bucket listing successful" || echo "‚ö† Bucket listing failed"
          
          # Try to create Knowledge Base with detailed error capture
          set +e  # Temporarily disable exit on error
          echo "Executing create-knowledge-base command..."
          KB_CREATE_OUTPUT=$(aws bedrock-agent create-knowledge-base \
            --name "${PROJECT_NAME}-knowledge-base" \
            --description "Knowledge base for America's Blood Centers chatbot with BDA parsing" \
            --role-arn "$KB_ROLE_ARN" \
            --knowledge-base-configuration '{
              "type": "VECTOR",
              "vectorKnowledgeBaseConfiguration": {
                "embeddingModelArn": "arn:aws:bedrock:'"$CDK_DEFAULT_REGION"'::foundation-model/'"$EMBEDDING_MODEL_ID"'",
                "supplementalDataStorageConfiguration": {
                  "storageLocations": [
                    {
                      "type": "S3",
                      "s3Location": {
                        "uri": "s3://'"$SUPPLEMENTAL_BUCKET"'"
                      }
                    }
                  ]
                }
              }
            }' \
            --storage-configuration '{
              "type": "OPENSEARCH_SERVERLESS",
              "opensearchServerlessConfiguration": {
                "collectionArn": "'"$COLLECTION_ARN"'",
                "vectorIndexName": "bedrock-knowledge-base-default-index",
                "fieldMapping": {
                  "vectorField": "bedrock-knowledge-base-default-vector",
                  "textField": "AMAZON_BEDROCK_TEXT_CHUNK",
                  "metadataField": "AMAZON_BEDROCK_METADATA"
                }
              }
            }' \
            --region "$CDK_DEFAULT_REGION" 2>&1)
          KB_EXIT_CODE=$?
          set -e  # Re-enable exit on error
          
          echo "Command completed."
          echo "üîç Debug: KB creation exit code: $KB_EXIT_CODE"
          echo "üîç Debug: KB creation output length: ${#KB_CREATE_OUTPUT}"
          echo "üîç Debug: KB creation output: $KB_CREATE_OUTPUT"
          
          if [ $KB_EXIT_CODE -eq 0 ]; then
            KB_ID=$(echo "$KB_CREATE_OUTPUT" | jq -r '.knowledgeBase.knowledgeBaseId // empty')
            if [ -n "$KB_ID" ] && [ "$KB_ID" != "null" ] && [ "$KB_ID" != "empty" ]; then
              echo "‚úì Knowledge Base created with supplemental storage: $KB_ID"
              echo "‚úì Supplemental storage configured: s3://$SUPPLEMENTAL_BUCKET/"
              echo "‚úì Ready for Bedrock Data Automation parsing of PDFs"
            else
              echo "‚ö† Knowledge Base creation succeeded but couldn't extract ID"
              echo "Full output: $KB_CREATE_OUTPUT"
              # Try to extract ID differently
              KB_ID=$(echo "$KB_CREATE_OUTPUT" | grep -o '"knowledgeBaseId":"[^"]*"' | cut -d'"' -f4)
              echo "Extracted KB ID: $KB_ID"
            fi
          else
            echo "‚úó Failed to create Knowledge Base with supplemental storage"
            echo "Exit code: $KB_EXIT_CODE"
            echo "Error output: $KB_CREATE_OUTPUT"
            echo "Collection ARN: $COLLECTION_ARN"
            echo "KB Role ARN: $KB_ROLE_ARN"
            echo "Supplemental Bucket: $SUPPLEMENTAL_BUCKET"
            exit 1
          fi
          else
            echo "‚úì Using existing Knowledge Base with supplemental storage: $KB_ID"
          fi
          
          # Step 6: Create Data Sources (S3 for PDFs + Web Crawler for Websites)
          echo "=== PHASE 4: Creating Dual Data Sources ==="
          BUCKET_ARN="arn:aws:s3:::${DOCUMENTS_BUCKET}"
          
          # Data Source 1: S3 for PDFs with Bedrock Data Automation Parser
          echo "Creating S3 Data Source for PDFs..."
          S3_DS_ID=$(aws bedrock-agent list-data-sources \
            --knowledge-base-id "$KB_ID" \
            --region "$CDK_DEFAULT_REGION" \
            --query "dataSourceSummaries[?name=='blood-centers-pdfs'].dataSourceId | [0]" \
            --output text 2>/dev/null || echo "")
          
          if [ -n "$S3_DS_ID" ] && [ "$S3_DS_ID" != "None" ] && [ "$S3_DS_ID" != "null" ]; then
            echo "‚úì Found existing S3 Data Source: $S3_DS_ID"
          else
            echo "Creating new S3 Data Source for PDFs..."
            S3_DS_ID=$(aws bedrock-agent create-data-source \
              --name "blood-centers-pdfs" \
              --description "PDF documents about blood donation with Bedrock Data Automation parser" \
              --knowledge-base-id "$KB_ID" \
              --data-deletion-policy RETAIN \
              --data-source-configuration '{
                "type": "S3",
                "s3Configuration": {
                  "bucketArn": "'"$BUCKET_ARN"'",
                  "inclusionPrefixes": ["pdfs/"]
                }
              }' \
              --vector-ingestion-configuration '{
                "chunkingConfiguration": {
                  "chunkingStrategy": "FIXED_SIZE",
                  "fixedSizeChunkingConfiguration": {
                    "maxTokens": 300,
                    "overlapPercentage": 20
                  }
                },
                "parsingConfiguration": {
                  "parsingStrategy": "BEDROCK_DATA_AUTOMATION",
                  "bedrockDataAutomationConfiguration": {
                    "parsingModality": "MULTIMODAL"
                  }
                }
              }' \
              --region "$CDK_DEFAULT_REGION" \
              --query 'dataSource.dataSourceId' --output text)
            
            if [ -z "$S3_DS_ID" ] || [ "$S3_DS_ID" = "None" ]; then
              echo "‚úó Failed to create S3 Data Source"
              exit 1
            fi
            
            echo "‚úì S3 Data Source created: $S3_DS_ID"
          fi
          
          # Data Source 2: Web Crawler for Websites
          echo "Creating Web Crawler Data Source..."
          WEB_DS_ID=$(aws bedrock-agent list-data-sources \
            --knowledge-base-id "$KB_ID" \
            --region "$CDK_DEFAULT_REGION" \
            --query "dataSourceSummaries[?name=='blood-centers-websites'].dataSourceId | [0]" \
            --output text 2>/dev/null || echo "")
          
          if [ -n "$WEB_DS_ID" ] && [ "$WEB_DS_ID" != "None" ] && [ "$WEB_DS_ID" != "null" ]; then
            echo "‚úì Found existing Web Crawler Data Source: $WEB_DS_ID"
          else
            echo "Creating new Web Crawler Data Source..."
            
            # Read URLs from S3 files dynamically
            echo "Reading URLs from S3 files..."
            SEED_URLS=""
            
            # Read daily-sync.txt for daily URLs
            if aws s3 ls "s3://$DOCUMENTS_BUCKET/daily-sync.txt" >/dev/null 2>&1; then
              DAILY_URLS=$(aws s3 cp "s3://$DOCUMENTS_BUCKET/daily-sync.txt" - | grep -v '^#' | grep -v '^$')
              echo "Found daily sync URLs:"
              echo "$DAILY_URLS"
            fi
            
            # Read urls.txt for regular URLs
            if aws s3 ls "s3://$DOCUMENTS_BUCKET/urls.txt" >/dev/null 2>&1; then
              REGULAR_URLS=$(aws s3 cp "s3://$DOCUMENTS_BUCKET/urls.txt" - | grep -v '^#' | grep -v '^$')
              echo "Found regular URLs:"
              echo "$REGULAR_URLS"
            fi
            
            # Combine URLs and format for JSON
            ALL_URLS=$(printf "%s\n%s\n" "$DAILY_URLS" "$REGULAR_URLS" | grep -v '^$' | grep -v '^#' | sort -u)
            
            if [ -n "$ALL_URLS" ]; then
              # Convert URLs to JSON format (remove trailing comma properly)
              SEED_URLS=$(echo "$ALL_URLS" | sed 's/^/{"url": "/' | sed 's/$/"},/' | tr '\n' ' ')
              # Remove the trailing comma and space
              SEED_URLS=$(echo "$SEED_URLS" | sed 's/, *$//')
              echo "Formatted seed URLs: $SEED_URLS"
            else
              # Fallback to default URLs if files not found
              echo "‚ö† No URL files found, using default URLs"
              SEED_URLS='{"url": "https://americasblood.org/for-donors/americas-blood-supply/"},
                        {"url": "https://americasblood.org/for-donors/find-a-blood-center/"},
                        {"url": "https://americasblood.org/news/"},
                        {"url": "https://americasblood.org/newsroom/"},
                        {"url": "https://americasblood.org/one-pagers-faqs/"}'
            fi
            
            WEB_DS_ID=$(aws bedrock-agent create-data-source \
              --name "blood-centers-websites" \
              --description "Blood donation websites and news from America's Blood Centers" \
              --knowledge-base-id "$KB_ID" \
              --data-deletion-policy RETAIN \
              --data-source-configuration '{
                "type": "WEB",
                "webConfiguration": {
                  "sourceConfiguration": {
                    "urlConfiguration": {
                      "seedUrls": ['"$SEED_URLS"']
                    }
                  },
                  "crawlerConfiguration": {
                    "crawlerLimits": {
                      "rateLimit": 300,
                      "maxPages": 50
                    },
                    "exclusionFilters": [
                      ".*/wp-admin/.*", 
                      ".*/login/.*", 
                      ".*/admin/.*",
                      ".*/wp-content/uploads/.*"
                    ],
                    "inclusionFilters": [
                      ".*/for-donors/.*", 
                      ".*/news/.*", 
                      ".*/newsroom/.*",
                      ".*/one-pagers-faqs/.*"
                    ]
                  }
                }
              }' \
              --vector-ingestion-configuration '{
                "chunkingConfiguration": {
                  "chunkingStrategy": "FIXED_SIZE",
                  "fixedSizeChunkingConfiguration": {
                    "maxTokens": 300,
                    "overlapPercentage": 20
                  }
                },
                "parsingConfiguration": {
                  "parsingStrategy": "BEDROCK_DATA_AUTOMATION",
                  "bedrockDataAutomationConfiguration": {
                    "parsingModality": "MULTIMODAL"
                  }
                }
              }' \
              --region "$CDK_DEFAULT_REGION" \
              --query 'dataSource.dataSourceId' --output text)
            
            if [ -z "$WEB_DS_ID" ] || [ "$WEB_DS_ID" = "None" ]; then
              echo "‚úó Failed to create Web Crawler Data Source"
              exit 1
            fi
            
            echo "‚úì Web Crawler Data Source created: $WEB_DS_ID"
          fi
          
          # Set primary data source ID for backward compatibility
          DS_ID="$S3_DS_ID"
          
          # Step 7: Document upload handled by CDK
          echo "=== PHASE 5: Document Upload ==="
          echo "‚úì Documents already uploaded by CDK BucketDeployment"
          echo "‚úì PDFs deployed to s3://$DOCUMENTS_BUCKET/pdfs/"
          echo "‚úì Text files deployed to s3://$DOCUMENTS_BUCKET/ (root level)"
          
          echo "‚úì Initial documents uploaded"
          
          # Step 8: Update Lambda environment variables
          echo "=== PHASE 6: Lambda Configuration ==="
          
          # Update chat Lambda
          CHAT_ENV=$(aws lambda get-function-configuration \
            --function-name "$CHAT_LAMBDA_NAME" \
            --region "$CDK_DEFAULT_REGION" \
            --query 'Environment.Variables' \
            --output json)
          
          echo "$CHAT_ENV" | jq \
            --arg kb_id "$KB_ID" \
            '{Variables: (. + {KNOWLEDGE_BASE_ID: $kb_id})}' > /tmp/chat_env.json
          
          aws lambda update-function-configuration \
            --function-name "$CHAT_LAMBDA_NAME" \
            --environment file:///tmp/chat_env.json \
            --region "$CDK_DEFAULT_REGION" >/dev/null
          
          # Update data ingestion Lambda
          DATA_ENV=$(aws lambda get-function-configuration \
            --function-name "$DATA_INGESTION_LAMBDA_NAME" \
            --region "$CDK_DEFAULT_REGION" \
            --query 'Environment.Variables' \
            --output json)
          
          echo "$DATA_ENV" | jq \
            --arg kb_id "$KB_ID" \
            --arg s3_ds_id "$S3_DS_ID" \
            --arg web_ds_id "$WEB_DS_ID" \
            '{Variables: (. + {KNOWLEDGE_BASE_ID: $kb_id, S3_DATA_SOURCE_ID: $s3_ds_id, WEB_DATA_SOURCE_ID: $web_ds_id, DATA_SOURCE_ID: $s3_ds_id})}' > /tmp/data_env.json
          
          aws lambda update-function-configuration \
            --function-name "$DATA_INGESTION_LAMBDA_NAME" \
            --environment file:///tmp/data_env.json \
            --region "$CDK_DEFAULT_REGION" >/dev/null
          
          echo "‚úì Lambda environment variables updated"
          
          # Step 9: Deploy Frontend (before ingestion to save time)
          echo "=== PHASE 7: Frontend Deployment ==="
          
          # Check if we're in a CodeBuild environment and have access to the Frontend directory
          if [ -d "../Frontend" ]; then
            echo "Deploying React frontend to AWS Amplify..."
            
            # Debug: Show current directory and Frontend structure
            echo "üîç Debug: Current directory: $(pwd)"
            echo "üîç Debug: Checking Frontend directory..."
            ls -la ../Frontend/ || echo "Frontend directory not accessible"
            echo "üîç Debug: Checking Frontend/public directory..."
            ls -la ../Frontend/public/ || echo "Frontend/public directory not accessible"
            echo "üîç Debug: Checking for index.html specifically..."
            [ -f "../Frontend/public/index.html" ] && echo "‚úì index.html found" || echo "‚úó index.html NOT found"
            
            # Additional debugging for file permissions and content
            echo "üîç Debug: File permissions check..."
            ls -la ../Frontend/public/index.html 2>/dev/null || echo "Cannot access index.html file"
            echo "üîç Debug: Directory permissions..."
            ls -ld ../Frontend/public/ 2>/dev/null || echo "Cannot access public directory"
            
            # Try to read the file to verify it's accessible
            echo "üîç Debug: Attempting to read index.html..."
            head -5 ../Frontend/public/index.html 2>/dev/null && echo "‚úì File is readable" || echo "‚úó File is not readable"
            
            # Create Amplify app
            AMPLIFY_APP_NAME="${PROJECT_NAME}-chatbot"
            echo "Creating Amplify app: $AMPLIFY_APP_NAME"
            
            # Check if app already exists
            EXISTING_APP_ID=$(aws amplify list-apps --query "apps[?name=='$AMPLIFY_APP_NAME'].appId | [0]" --output text 2>/dev/null || echo "")
            
            if [ -n "$EXISTING_APP_ID" ] && [ "$EXISTING_APP_ID" != "None" ] && [ "$EXISTING_APP_ID" != "null" ]; then
              echo "‚úì Found existing Amplify app: $EXISTING_APP_ID"
              AMPLIFY_APP_ID="$EXISTING_APP_ID"
            else
              echo "Creating new Amplify app..."
              AMPLIFY_APP_ID=$(aws amplify create-app \
                --name "$AMPLIFY_APP_NAME" \
                --platform WEB \
                --environment-variables "REACT_APP_API_BASE_URL=$API_URL,REACT_APP_CHAT_ENDPOINT=$API_URL" \
                --query 'app.appId' --output text)
              echo "‚úì Amplify app created: $AMPLIFY_APP_ID"
            fi
            
            # Create main branch if it doesn't exist
            EXISTING_BRANCH=$(aws amplify list-branches --app-id "$AMPLIFY_APP_ID" --query "branches[?branchName=='main'].branchName | [0]" --output text 2>/dev/null || echo "")
            
            if [ -z "$EXISTING_BRANCH" ] || [ "$EXISTING_BRANCH" = "None" ] || [ "$EXISTING_BRANCH" = "null" ]; then
              echo "Creating main branch..."
              aws amplify create-branch \
                --app-id "$AMPLIFY_APP_ID" \
                --branch-name "main" \
                --stage PRODUCTION
              echo "‚úì Main branch created"
            else
              echo "‚úì Main branch already exists"
            fi
            
            # Build and deploy frontend
            echo "Building React frontend..."
            cd ../Frontend
            
            # Ensure index.html exists and is accessible
            if [ ! -f "public/index.html" ] || [ ! -r "public/index.html" ]; then
              echo "‚ö† index.html missing or not readable, creating it..."
              mkdir -p public
              echo '<!DOCTYPE html>' > public/index.html
              echo '<html lang="en">' >> public/index.html
              echo '  <head>' >> public/index.html
              echo '    <meta charset="utf-8" />' >> public/index.html
              echo '    <meta name="viewport" content="width=device-width, initial-scale=1" />' >> public/index.html
              echo '    <meta name="theme-color" content="#B71C1C" />' >> public/index.html
              echo '    <meta name="description" content="America'\''s Blood Centers AI Assistant" />' >> public/index.html
              echo '    <link rel="manifest" href="%PUBLIC_URL%/manifest.json" />' >> public/index.html
              echo '    <title>America'\''s Blood Centers - AI Assistant</title>' >> public/index.html
              echo '  </head>' >> public/index.html
              echo '  <body>' >> public/index.html
              echo '    <noscript>You need to enable JavaScript to run this app.</noscript>' >> public/index.html
              echo '    <div id="root"></div>' >> public/index.html
              echo '  </body>' >> public/index.html
              echo '</html>' >> public/index.html
              echo "‚úì index.html created"
            else
              echo "‚úì index.html exists and is readable"
            fi
            
            # Ensure manifest.json exists
            if [ ! -f "public/manifest.json" ]; then
              echo "‚ö† manifest.json missing, creating it..."
              echo '{' > public/manifest.json
              echo '  "short_name": "ABC Assistant",' >> public/manifest.json
              echo '  "name": "America'\''s Blood Centers AI Assistant",' >> public/manifest.json
              echo '  "start_url": ".",' >> public/manifest.json
              echo '  "display": "standalone",' >> public/manifest.json
              echo '  "theme_color": "#B71C1C",' >> public/manifest.json
              echo '  "background_color": "#ffffff"' >> public/manifest.json
              echo '}' >> public/manifest.json
              echo "‚úì manifest.json created"
            fi
            
            # Create environment file with API URL
            echo "REACT_APP_API_BASE_URL=$API_URL" > .env.production
            echo "REACT_APP_CHAT_ENDPOINT=$API_URL" >> .env.production
            
            # Install dependencies and build
            echo "Installing npm dependencies..."
            npm install --production --no-audit --no-fund
            
            echo "Building React application..."
            set +e  # Temporarily disable exit on error
            BUILD_OUTPUT=$(npm run build 2>&1)
            BUILD_EXIT_CODE=$?
            set -e  # Re-enable exit on error
            
            echo "üîç Debug: Build exit code: $BUILD_EXIT_CODE"
            
            if [ $BUILD_EXIT_CODE -eq 0 ]; then
              echo "‚úì React build successful"
            else
              echo "‚úó React build failed"
              echo "Build output: $BUILD_OUTPUT"
              echo "üìã Attempting manual build recovery..."
              
              # Try to create a minimal build directory
              mkdir -p build
              echo '<!DOCTYPE html>' > build/index.html
              echo '<html lang="en">' >> build/index.html
              echo '<head>' >> build/index.html
              echo '    <meta charset="utf-8" />' >> build/index.html
              echo '    <meta name="viewport" content="width=device-width, initial-scale=1" />' >> build/index.html
              echo '    <title>America'\''s Blood Centers - AI Assistant</title>' >> build/index.html
              echo '</head>' >> build/index.html
              echo '<body>' >> build/index.html
              echo '    <div id="root">' >> build/index.html
              echo '        <h1>America'\''s Blood Centers AI Assistant</h1>' >> build/index.html
              echo '        <p>The application is being deployed. Please check back shortly.</p>' >> build/index.html
              echo '        <p>API Endpoint: <code>'$API_URL'</code></p>' >> build/index.html
              echo '    </div>' >> build/index.html
              echo '</body>' >> build/index.html
              echo '</html>' >> build/index.html
              echo "‚úì Created fallback build"
            fi
            
            # Create deployment package
            cd build
            zip -r ../build.zip .
            cd ..
            
            # Upload to S3 for Amplify deployment using CDK-created bucket
            BUILD_BUCKET=$(aws cloudformation describe-stacks \
              --stack-name "$STACK_NAME" \
              --query "Stacks[0].Outputs[?OutputKey=='BuildsBucketName'].OutputValue | [0]" \
              --output text)
            
            if [ -n "$BUILD_BUCKET" ] && [ "$BUILD_BUCKET" != "None" ]; then
              echo "Uploading build to CDK-created S3 bucket: $BUILD_BUCKET"
              BUILD_KEY="builds/build-$(date +%s).zip"
              aws s3 cp build.zip "s3://$BUILD_BUCKET/$BUILD_KEY"
              echo "‚úì Build uploaded to S3"
            else
              echo "‚ö† Builds bucket not found, using fallback"
              BUILD_BUCKET="${PROJECT_NAME}-builds-${AWS_ACCOUNT_ID}-${CDK_DEFAULT_REGION}"
              BUILD_KEY="builds/build-$(date +%s).zip"
              aws s3 cp build.zip "s3://$BUILD_BUCKET/$BUILD_KEY" 2>/dev/null || echo "‚ö† Could not upload to S3"
            fi
            
            # Update AmplifyDeployer Lambda with Amplify app details (like Catholic Charity project)
            echo "Configuring AmplifyDeployer Lambda for automatic deployment..."
            
            # Get AmplifyDeployer function name from CDK outputs
            AMPLIFY_DEPLOYER_FUNCTION_NAME=$(aws cloudformation describe-stacks \
              --stack-name "$STACK_NAME" \
              --query "Stacks[0].Outputs[?OutputKey=='AmplifyDeployerFunctionName'].OutputValue | [0]" \
              --output text 2>/dev/null || echo "")
            
            if [ -n "$AMPLIFY_DEPLOYER_FUNCTION_NAME" ] && [ "$AMPLIFY_DEPLOYER_FUNCTION_NAME" != "None" ]; then
              echo "‚úì Found AmplifyDeployer Lambda: $AMPLIFY_DEPLOYER_FUNCTION_NAME"
              
              # Wait for Lambda function to be ready (it might still be updating)
              echo "Waiting for Lambda function to be ready..."
              LAMBDA_READY=false
              WAIT_COUNT=0
              MAX_WAIT=60  # Increased to 60 seconds for large Lambda packages
              
              while [ "$LAMBDA_READY" = false ] && [ $WAIT_COUNT -lt $MAX_WAIT ]; do
                LAMBDA_STATE=$(aws lambda get-function \
                  --function-name "$AMPLIFY_DEPLOYER_FUNCTION_NAME" \
                  --region "$CDK_DEFAULT_REGION" \
                  --query 'Configuration.LastUpdateStatus' \
                  --output text 2>/dev/null || echo "ERROR")
                
                echo "üîç Debug: Lambda state: $LAMBDA_STATE (waited ${WAIT_COUNT}s)"
                
                if [ "$LAMBDA_STATE" = "Successful" ]; then
                  LAMBDA_READY=true
                  echo "‚úì Lambda function is ready"
                elif [ "$LAMBDA_STATE" = "Failed" ]; then
                  echo "‚úó Lambda function deployment failed"
                  # Get more details about the failure
                  aws lambda get-function \
                    --function-name "$AMPLIFY_DEPLOYER_FUNCTION_NAME" \
                    --region "$CDK_DEFAULT_REGION" \
                    --query 'Configuration.LastUpdateStatusReason' \
                    --output text
                  break
                else
                  echo "‚è≥ Lambda function status: $LAMBDA_STATE (waiting ${WAIT_COUNT}s)"
                  sleep 3
                  WAIT_COUNT=$((WAIT_COUNT + 3))
                fi
              done
              
              if [ "$LAMBDA_READY" = false ]; then
                echo "‚ö† Lambda function not ready after ${MAX_WAIT}s"
                echo "Current state: $LAMBDA_STATE"
                echo "Proceeding anyway, but deployment may fail..."
              fi
              
              # Update AmplifyDeployer Lambda with Amplify app details (Catholic Charity style)
              echo "Updating Lambda environment variables..."
              aws lambda update-function-configuration \
                --function-name "$AMPLIFY_DEPLOYER_FUNCTION_NAME" \
                --environment "Variables={AMPLIFY_APP_ID=$AMPLIFY_APP_ID,AMPLIFY_BRANCH_NAME=main}" \
                --region "$CDK_DEFAULT_REGION"
              
              # Wait a moment for the environment update to propagate
              echo "Waiting for environment update to propagate..."
              sleep 5
              
              echo "‚úì AmplifyDeployer configured with app ID: $AMPLIFY_APP_ID"
              
              # Trigger immediate deployment (like Catholic Charity project)
              echo "üöÄ Triggering immediate Amplify deployment..."
              
              # Create JSON payload for Lambda invocation
              DEPLOY_PAYLOAD='{"bucket":"'$BUILD_BUCKET'","key":"'$BUILD_KEY'"}'
              
              echo "Invoking AmplifyDeployer Lambda with payload: $DEPLOY_PAYLOAD"
              
              # Use --cli-binary-format raw-in-base64-out for proper JSON handling
              set +e  # Temporarily disable exit on error
              DEPLOY_RESULT=$(aws lambda invoke \
                --function-name "$AMPLIFY_DEPLOYER_FUNCTION_NAME" \
                --payload "$DEPLOY_PAYLOAD" \
                --cli-binary-format raw-in-base64-out \
                --region "$CDK_DEFAULT_REGION" \
                /tmp/deploy_response.json 2>&1)
              DEPLOY_EXIT_CODE=$?
              set -e  # Re-enable exit on error
              
              echo "üîç Debug: Lambda invocation exit code: $DEPLOY_EXIT_CODE"
              echo "üîç Debug: Lambda invocation result: $DEPLOY_RESULT"
              
              if [ $DEPLOY_EXIT_CODE -eq 0 ]; then
                if [ -f "/tmp/deploy_response.json" ]; then
                  DEPLOY_RESPONSE=$(cat /tmp/deploy_response.json)
                  echo "üîç Debug: Lambda response: $DEPLOY_RESPONSE"
                  
                  # Check if the response contains an error
                  if echo "$DEPLOY_RESPONSE" | grep -q '"errorMessage"'; then
                    echo "‚úó Lambda execution failed"
                    echo "Error: $DEPLOY_RESPONSE"
                    FRONTEND_URL="https://main.$AMPLIFY_APP_ID.amplifyapp.com (Lambda execution failed)"
                  elif echo "$DEPLOY_RESPONSE" | grep -q '"statusCode": 200'; then
                    echo "‚úì Amplify deployment triggered successfully"
                    echo "üì± Frontend will be available at: https://main.$AMPLIFY_APP_ID.amplifyapp.com"
                    echo "‚è±Ô∏è Deployment typically completes in 2-3 minutes"
                    FRONTEND_URL="https://main.$AMPLIFY_APP_ID.amplifyapp.com"
                  else
                    echo "‚ö† Deployment trigger may have failed"
                    echo "Response: $DEPLOY_RESPONSE"
                    
                    # Try direct Amplify deployment as fallback
                    echo "üîÑ Attempting direct Amplify deployment as fallback..."
                    echo "üîç Debug: Using CodeBuild's broader permissions for direct deployment"
                    
                    set +e  # Temporarily disable exit on error
                    DIRECT_DEPLOY_RESULT=$(aws amplify start-deployment \
                      --app-id "$AMPLIFY_APP_ID" \
                      --branch-name "main" \
                      --source-url "s3://$BUILD_BUCKET/$BUILD_KEY" \
                      --region "$CDK_DEFAULT_REGION" 2>&1)
                    DIRECT_DEPLOY_EXIT_CODE=$?
                    set -e  # Re-enable exit on error
                    
                    echo "üîç Debug: Direct deployment exit code: $DIRECT_DEPLOY_EXIT_CODE"
                    echo "üîç Debug: Direct deployment result: $DIRECT_DEPLOY_RESULT"
                    
                    if [ $DIRECT_DEPLOY_EXIT_CODE -eq 0 ]; then
                      DIRECT_JOB_ID=$(echo "$DIRECT_DEPLOY_RESULT" | jq -r '.jobSummary.jobId // empty')
                      if [ -n "$DIRECT_JOB_ID" ] && [ "$DIRECT_JOB_ID" != "empty" ]; then
                        echo "‚úì Direct Amplify deployment started successfully: $DIRECT_JOB_ID"
                        echo "üì± Frontend will be available at: https://main.$AMPLIFY_APP_ID.amplifyapp.com"
                        echo "‚è±Ô∏è Deployment typically completes in 2-3 minutes"
                        FRONTEND_URL="https://main.$AMPLIFY_APP_ID.amplifyapp.com"
                      else
                        echo "‚ö† Direct deployment succeeded but couldn't extract job ID"
                        FRONTEND_URL="https://main.$AMPLIFY_APP_ID.amplifyapp.com (check Amplify console)"
                      fi
                    else
                      echo "‚úó Direct deployment also failed: $DIRECT_DEPLOY_RESULT"
                      echo "This suggests a broader issue with Amplify deployment"
                      FRONTEND_URL="https://main.$AMPLIFY_APP_ID.amplifyapp.com (deployment failed)"
                    fi
                  fi
                else
                  echo "‚úó No response file created"
                  FRONTEND_URL="https://main.$AMPLIFY_APP_ID.amplifyapp.com (no response file)"
                fi
              else
                echo "‚úó Failed to invoke Lambda function"
                echo "Error: $DEPLOY_RESULT"
                echo "Exit code: $DEPLOY_EXIT_CODE"
                FRONTEND_URL="https://main.$AMPLIFY_APP_ID.amplifyapp.com (Lambda invocation failed)"
              fi
            else
              echo "‚ö† AmplifyDeployer Lambda not found, falling back to manual deployment"
              echo "üìã Manual deployment options:"
              echo "1. AWS Amplify Console: https://console.aws.amazon.com/amplify/home#/$AMPLIFY_APP_ID"
              echo "2. Use the uploaded build: s3://$BUILD_BUCKET/$BUILD_KEY"
              echo "3. Or deploy manually with AWS CLI:"
              echo "   aws amplify start-deployment --app-id $AMPLIFY_APP_ID --branch-name main --source-url s3://$BUILD_BUCKET/$BUILD_KEY"
              FRONTEND_URL="https://main.$AMPLIFY_APP_ID.amplifyapp.com (manual deployment required)"
            fi
            
            cd ../Backend
          else
            echo "‚ö† Frontend directory not found. Skipping frontend deployment."
            echo "üìã To deploy frontend manually:"
            echo "1. Navigate to the Frontend directory"
            echo "2. Set REACT_APP_API_BASE_URL=$API_URL"
            echo "3. Run: npm install && npm run build"
            echo "4. Deploy to AWS Amplify"
            FRONTEND_URL="Manual deployment required"
          fi
          
          # Step 10: Start sequential ingestion jobs (PDFs first, then websites)
          echo ""
          echo "=== PHASE 8: Knowledge Base Ingestion ==="
          
          # Start S3 ingestion job for PDFs (faster, so we do this first)
          echo "Starting S3 ingestion job for PDFs..."
          S3_INGESTION_JOB_ID=$(aws bedrock-agent start-ingestion-job \
            --knowledge-base-id "$KB_ID" \
            --data-source-id "$S3_DS_ID" \
            --description "Initial ingestion of PDF documents" \
            --region "$CDK_DEFAULT_REGION" \
            --query 'ingestionJob.ingestionJobId' --output text)
          
          echo "‚úì S3 ingestion job started: $S3_INGESTION_JOB_ID"
          
          # Wait for S3 ingestion to complete before starting web crawler
          echo "Waiting for S3 ingestion to complete before starting web crawler..."
          S3_JOB_STATUS="IN_PROGRESS"
          WAIT_TIME=0
          MAX_WAIT=600  # 10 minutes max wait
          
          while [ "$S3_JOB_STATUS" = "IN_PROGRESS" ] && [ $WAIT_TIME -lt $MAX_WAIT ]; do
            sleep 30
            WAIT_TIME=$((WAIT_TIME + 30))
            
            S3_JOB_STATUS=$(aws bedrock-agent get-ingestion-job \
              --knowledge-base-id "$KB_ID" \
              --data-source-id "$S3_DS_ID" \
              --ingestion-job-id "$S3_INGESTION_JOB_ID" \
              --region "$CDK_DEFAULT_REGION" \
              --query 'ingestionJob.status' --output text)
            
            echo "S3 ingestion status: $S3_JOB_STATUS (waited ${WAIT_TIME}s)"
          done
          
          if [ "$S3_JOB_STATUS" = "COMPLETE" ]; then
            echo "‚úì S3 ingestion completed successfully"
            
            # Now start Web Crawler ingestion job
            echo "Starting Web Crawler ingestion job..."
            WEB_INGESTION_JOB_ID=$(aws bedrock-agent start-ingestion-job \
              --knowledge-base-id "$KB_ID" \
              --data-source-id "$WEB_DS_ID" \
              --description "Initial crawling of America's Blood Centers websites" \
              --region "$CDK_DEFAULT_REGION" \
              --query 'ingestionJob.ingestionJobId' --output text)
            
            echo "‚úì Web Crawler ingestion job started: $WEB_INGESTION_JOB_ID"
          else
            echo "‚ö† S3 ingestion did not complete in time (status: $S3_JOB_STATUS)"
            echo "‚ö† Skipping web crawler ingestion for now"
            WEB_INGESTION_JOB_ID="SKIPPED"
          fi
          
          # Set primary ingestion job ID for backward compatibility
          INGESTION_JOB_ID="$S3_INGESTION_JOB_ID"
          
          # Step 11: Update Daily Sync Lambda (already created by CDK)
          echo ""
          echo "=== PHASE 9: Daily Sync Configuration ==="
          
          # Update the existing data ingestion Lambda with Knowledge Base IDs
          DATA_INGESTION_FUNCTION_NAME=$(aws cloudformation describe-stacks \
            --stack-name "$STACK_NAME" \
            --query "Stacks[0].Outputs[?OutputKey=='DataIngestionFunctionName'].OutputValue | [0]" \
            --output text)
          
          if [ -n "$DATA_INGESTION_FUNCTION_NAME" ] && [ "$DATA_INGESTION_FUNCTION_NAME" != "None" ]; then
            echo "Updating data ingestion Lambda function: $DATA_INGESTION_FUNCTION_NAME"
            
            # Update environment variables with actual Knowledge Base and Data Source IDs
            aws lambda update-function-configuration \
              --function-name "$DATA_INGESTION_FUNCTION_NAME" \
              --environment "Variables={DOCUMENTS_BUCKET=$DOCUMENTS_BUCKET,KNOWLEDGE_BASE_ID=$KB_ID,S3_DATA_SOURCE_ID=$S3_DS_ID,WEB_DATA_SOURCE_ID=$WEB_DS_ID}" \
              --region "$CDK_DEFAULT_REGION"
            
            echo "‚úì Data ingestion Lambda updated with Knowledge Base IDs"
            echo "‚úì Daily sync is already configured via CDK EventBridge rule"
            echo "‚úì Schedule: Daily at 7 PM UTC (2 PM EST)"
            
            # Check if daily-sync.txt exists for URL configuration
            if aws s3 ls "s3://$DOCUMENTS_BUCKET/daily-sync.txt" >/dev/null 2>&1; then
              echo "‚úì Found daily-sync.txt - URLs will be read dynamically"
            else
              echo "üìã To configure daily sync URLs:"
              echo "1. Upload daily-sync.txt to s3://$DOCUMENTS_BUCKET/"
              echo "2. Add URLs (one per line) that should be synced daily"
              echo "3. The Lambda function will read these URLs automatically"
            fi
          else
            echo "‚ö† Data ingestion Lambda function not found in stack outputs"
            echo "‚ö† Daily sync may not be properly configured"
          fi
          
          echo ""
          echo "========================================="
          echo "‚úÖ Deployment Complete!"
          echo "========================================="
          echo "üß† Knowledge Base ID: $KB_ID"
          echo "üìÑ S3 Data Source ID: $S3_DS_ID (PDFs with Bedrock Data Automation)"
          echo "üåê Web Crawler Data Source ID: $WEB_DS_ID (Websites)"
          echo "üìÅ Documents Bucket: $DOCUMENTS_BUCKET"
          echo "üîç Collection ARN: $COLLECTION_ARN"
          echo "üìä S3 Ingestion Job ID: $S3_INGESTION_JOB_ID"
          echo "üï∑Ô∏è Web Ingestion Job ID: $WEB_INGESTION_JOB_ID"
          echo "üåê API Gateway URL: $API_URL"
          echo "üíª Frontend URL: ${FRONTEND_URL:-Not deployed}"
          echo "üöÄ Deployment Type: Automated via EventBridge + AmplifyDeployer Lambda"
          echo "========================================="
          echo ""
          echo "========================================="
          echo "üìã Deployment Summary"
          echo "========================================="
          echo "üìÅ Documents Bucket: $DOCUMENTS_BUCKET"
          echo "üß† Knowledge Base ID: $KB_ID"
          echo "üìÑ S3 Data Source ID: $S3_DS_ID (PDFs)"
          echo "üåê Web Data Source ID: $WEB_DS_ID (Websites)"
          echo "üîó API Gateway URL: $API_URL"
          echo ""
          echo "========================================="
          echo "üöÄ Next Steps"
          echo "========================================="
          echo "1Ô∏è‚É£ Wait 5-10 minutes for ingestion jobs to complete"
          echo ""
          echo "2Ô∏è‚É£ Test the API:"
          echo "   curl -X POST $API_URL \\"
          echo "     -H 'Content-Type: application/json' \\"
          echo "     -d '{\"message\":\"How many people donate blood?\",\"language\":\"en\"}'"
          echo ""
          echo "3Ô∏è‚É£ Test Spanish support:"
          echo "   curl -X POST $API_URL \\"
          echo "     -H 'Content-Type: application/json' \\"
          echo "     -d '{\"message\":\"¬øCu√°ntas personas donan sangre?\",\"language\":\"es\"}'"
          echo ""
          echo "4Ô∏è‚É£ Monitor ingestion jobs:"
          echo "   # S3 Data Source (PDFs):"
          echo "   aws bedrock-agent get-ingestion-job \\"
          echo "     --knowledge-base-id $KB_ID \\"
          echo "     --data-source-id $S3_DS_ID \\"
          echo "     --ingestion-job-id $S3_INGESTION_JOB_ID \\"
          echo "     --region $CDK_DEFAULT_REGION"
          echo ""
          echo "   # Web Crawler Data Source:"
          echo "   aws bedrock-agent get-ingestion-job \\"
          echo "     --knowledge-base-id $KB_ID \\"
          echo "     --data-source-id $WEB_DS_ID \\"
          echo "     --ingestion-job-id $WEB_INGESTION_JOB_ID \\"
          echo "     --region $CDK_DEFAULT_REGION"
          echo ""
          echo "5Ô∏è‚É£ Add more documents:"
          echo "   # Add PDFs (uses Bedrock Data Automation parser):"
          echo "   aws s3 cp document.pdf s3://$DOCUMENTS_BUCKET/pdfs/"
          echo "   aws bedrock-agent start-ingestion-job \\"
          echo "     --knowledge-base-id $KB_ID \\"
          echo "     --data-source-id $S3_DS_ID \\"
          echo "     --region $CDK_DEFAULT_REGION"
          echo ""
          echo "   # Re-crawl websites (automatic parsing):"
          echo "   aws bedrock-agent start-ingestion-job \\"
          echo "     --knowledge-base-id $KB_ID \\"
          echo "     --data-source-id $WEB_DS_ID \\"
          echo "     --region $CDK_DEFAULT_REGION"
          echo ""
          echo "üìä Monitor resources:"
          echo "  ‚Ä¢ Knowledge Base: https://console.aws.amazon.com/bedrock/home?region=${CDK_DEFAULT_REGION}#/knowledge-bases"
          echo "  ‚Ä¢ OpenSearch: https://console.aws.amazon.com/aos/home?region=${CDK_DEFAULT_REGION}#opensearch/collections"
          echo "  ‚Ä¢ S3 Bucket: https://s3.console.aws.amazon.com/s3/buckets/$DOCUMENTS_BUCKET"
          echo ""
          echo "üí∞ Estimated monthly cost: \$8-20 (vs \$20+ for Q Business)"
          echo "üéØ Features: Dual data sources, bilingual support, automatic web crawling"
          echo ""
        fi

  post_build:
    commands:
      - echo "========================================="
      - echo "üéâ Deployment Complete"
      - echo "========================================="
      - |
        if [ "$ACTION" = "deploy" ]; then
          echo "‚úÖ Bedrock chatbot deployed successfully"
          echo "üìä Check CloudWatch Logs for Lambda execution details"
          echo "üß† Test Knowledge Base retrieval via the chat API"
          echo "üåê Monitor ingestion jobs in Bedrock console"
        else
          echo "‚úÖ Stack destroyed successfully"
        fi

artifacts:
  files:
    - "**/*"
  base-directory: "Backend/cdk.out"