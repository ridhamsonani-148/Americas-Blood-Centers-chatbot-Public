version: 0.2

phases:
  install:
    runtime-versions:
      nodejs: 20
      python: 3.11
    commands:
      - echo "Installing AWS CDK CLI..."
      - npm install -g aws-cdk@latest
      - cd Backend
      - npm install

  pre_build:
    commands:
      - echo "=== Building TypeScript ==="
      - npm run build
      - echo "‚úÖ TypeScript build completed"
      - echo "=== Installing Lambda dependencies ==="
      - cd lambda && pip install -r requirements.txt -t . && cd ..
      - echo "‚úÖ Lambda dependencies installed"
      - echo "=== Bootstrapping CDK Environment ==="
      - |
        # Set default PROJECT_NAME for bootstrap if not provided
        if [ -z "$PROJECT_NAME" ]; then
          PROJECT_NAME="abc"
        fi
        cdk bootstrap --require-approval never \
          --context projectName="$PROJECT_NAME" \
          --context modelId="${MODEL_ID:-anthropic.claude-3-haiku-20240307-v1:0}" \
          --context embeddingModelId="${EMBEDDING_MODEL_ID:-amazon.titan-embed-text-v1}"
      - echo "‚úÖ CDK Bootstrap completed"

  build:
    commands:
      - |
        if [ "$ACTION" = "destroy" ]; then
          echo "=== Destroying Stack ==="
          cdk destroy AmericasBloodCentersBedrockStack --force \
            --context projectName="$PROJECT_NAME" \
            --context modelId="$MODEL_ID" \
            --context embeddingModelId="$EMBEDDING_MODEL_ID"
          echo "‚úÖ Stack destroyed successfully"
        else
          echo "========================================="
          echo "Deploying America's Blood Centers Bedrock Chatbot"
          echo "========================================="
          
          echo "=== CDK Infrastructure Deployment ==="
          
          # Set default PROJECT_NAME if not provided
          if [ -z "$PROJECT_NAME" ]; then
            PROJECT_NAME="abc"
            echo "üîç Using default PROJECT_NAME: $PROJECT_NAME"
          else
            echo "üîç Using provided PROJECT_NAME: $PROJECT_NAME"
          fi
          
          cdk deploy AmericasBloodCentersBedrockStack --require-approval never \
            --context projectName="$PROJECT_NAME" \
            --context modelId="$MODEL_ID" \
            --context embeddingModelId="$EMBEDDING_MODEL_ID" \
            --outputs-file outputs.json
          
          if [ $? -ne 0 ]; then
            echo "‚ùå ERROR: CDK deployment failed"
            exit 1
          fi
          
          echo "‚úÖ CDK deployment successful"
          
          echo "=== Extracting CDK Outputs ==="
          STACK_NAME="AmericasBloodCentersBedrockStack"
          DOCUMENTS_BUCKET=$(cat outputs.json | jq -r '.AmericasBloodCentersBedrockStack.DocumentsBucketName // empty')
          KB_ID=$(cat outputs.json | jq -r '.AmericasBloodCentersBedrockStack.KnowledgeBaseId // empty')
          API_URL=$(cat outputs.json | jq -r '.AmericasBloodCentersBedrockStack.ApiGatewayUrl // empty')
          OPENSEARCH_ENDPOINT=$(cat outputs.json | jq -r '.AmericasBloodCentersBedrockStack.OpenSearchCollectionEndpoint // empty')
          
          # Discover data source IDs dynamically by name
          echo "Discovering data source IDs by name..."
          S3_DS_ID=$(aws bedrock-agent list-data-sources \
            --knowledge-base-id "$KB_ID" \
            --region "$CDK_DEFAULT_REGION" \
            --query 'dataSourceSummaries[?name==`BloodCentersDocuments-v2`].dataSourceId' \
            --output text)
          echo "Found S3 Data Source ID: $S3_DS_ID"
          
          WEB_DS_ID=$(aws bedrock-agent list-data-sources \
            --knowledge-base-id "$KB_ID" \
            --region "$CDK_DEFAULT_REGION" \
            --query 'dataSourceSummaries[?name==`BloodCentersWebsite-v2`].dataSourceId' \
            --output text)
          echo "Found Web Data Source ID: $WEB_DS_ID"
          
          DAILY_DS_ID=$(aws bedrock-agent list-data-sources \
            --knowledge-base-id "$KB_ID" \
            --region "$CDK_DEFAULT_REGION" \
            --query 'dataSourceSummaries[?name==`BloodCentersDailySync-v2`].dataSourceId' \
            --output text)
          echo "Found Daily Sync Data Source ID: $DAILY_DS_ID"
          
          if [ -z "$DOCUMENTS_BUCKET" ] || [ -z "$KB_ID" ] || [ -z "$API_URL" ]; then
            echo "‚ùå ERROR: Failed to extract required outputs from CDK deployment"
            echo "Documents Bucket: $DOCUMENTS_BUCKET"
            echo "Knowledge Base ID: $KB_ID"
            echo "API URL: $API_URL"
            exit 1
          fi
          
          # Check if data sources exist, if not they will be created by CDK
          if [ -z "$S3_DS_ID" ] || [ "$S3_DS_ID" = "None" ]; then
            echo "‚ö†Ô∏è S3 Data Source not found - will be created by CDK"
            S3_DS_ID=""
          fi
          
          if [ -z "$WEB_DS_ID" ] || [ "$WEB_DS_ID" = "None" ]; then
            echo "‚ö†Ô∏è Web Data Source not found - will be created by CDK"
            WEB_DS_ID=""
          fi
          
          if [ -z "$DAILY_DS_ID" ] || [ "$DAILY_DS_ID" = "None" ]; then
            echo "‚ö†Ô∏è Daily Sync Data Source not found - will be created by CDK"
            DAILY_DS_ID=""
          fi
          
          echo "‚úÖ Outputs extracted successfully"
          echo "  üìÅ Documents Bucket: $DOCUMENTS_BUCKET"
          echo "  üß† Knowledge Base ID: $KB_ID"
          echo "  üìÑ S3 Data Source ID: $S3_DS_ID"
          echo "  üåê Web Data Source ID: $WEB_DS_ID"
          echo "  üìÖ Daily Sync Data Source ID: $DAILY_DS_ID"
          echo "  üåê API Gateway URL: $API_URL"
          echo "  üîç OpenSearch Endpoint: $OPENSEARCH_ENDPOINT"
          
          echo ""
          echo "=== Starting Knowledge Base Ingestion ==="
          
          # Wait a moment for data sources to be fully created
          echo "Waiting for data sources to be ready..."
          sleep 30
          
          # Re-discover data source IDs after CDK deployment
          echo "Re-discovering data source IDs after CDK deployment..."
          S3_DS_ID=$(aws bedrock-agent list-data-sources \
            --knowledge-base-id "$KB_ID" \
            --region "$CDK_DEFAULT_REGION" \
            --query 'dataSourceSummaries[?name==`BloodCentersDocuments-v2`].dataSourceId' \
            --output text)
          
          WEB_DS_ID=$(aws bedrock-agent list-data-sources \
            --knowledge-base-id "$KB_ID" \
            --region "$CDK_DEFAULT_REGION" \
            --query 'dataSourceSummaries[?name==`BloodCentersWebsite-v2`].dataSourceId' \
            --output text)
          
          DAILY_DS_ID=$(aws bedrock-agent list-data-sources \
            --knowledge-base-id "$KB_ID" \
            --region "$CDK_DEFAULT_REGION" \
            --query 'dataSourceSummaries[?name==`BloodCentersDailySync-v2`].dataSourceId' \
            --output text)
          
          echo "Final Data Source IDs:"
          echo "  S3: $S3_DS_ID"
          echo "  Web: $WEB_DS_ID"
          echo "  Daily: $DAILY_DS_ID"
          
          # Start S3 ingestion job for PDFs
          if [ -n "$S3_DS_ID" ] && [ "$S3_DS_ID" != "None" ]; then
            echo "Starting S3 ingestion job for PDFs..."
            S3_INGESTION_JOB_ID=$(aws bedrock-agent start-ingestion-job \
              --knowledge-base-id "$KB_ID" \
              --data-source-id "$S3_DS_ID" \
              --description "Initial ingestion of PDF documents" \
              --region "$CDK_DEFAULT_REGION" \
              --query 'ingestionJob.ingestionJobId' --output text)
            
            echo "‚úì S3 ingestion job started: $S3_INGESTION_JOB_ID"
          else
            echo "‚ö†Ô∏è S3 Data Source not found, skipping S3 ingestion"
            S3_INGESTION_JOB_ID=""
          fi
          
          # Start Web Crawler ingestion job
          if [ -n "$WEB_DS_ID" ] && [ "$WEB_DS_ID" != "None" ]; then
            echo "Starting Web Crawler ingestion job..."
            WEB_INGESTION_JOB_ID=$(aws bedrock-agent start-ingestion-job \
              --knowledge-base-id "$KB_ID" \
              --data-source-id "$WEB_DS_ID" \
              --description "Initial crawling of America's Blood Centers websites" \
              --region "$CDK_DEFAULT_REGION" \
              --query 'ingestionJob.ingestionJobId' --output text)
            
            echo "‚úì Web Crawler ingestion job started: $WEB_INGESTION_JOB_ID"
          else
            echo "‚ö†Ô∏è Web Data Source not found, skipping Web ingestion"
            WEB_INGESTION_JOB_ID=""
          fi
          
          echo ""
          echo "=== Amplify App Setup ==="
          
          # Set up Amplify app name
          AMPLIFY_APP_NAME="${PROJECT_NAME}-chatbot"
          AMPLIFY_BRANCH_NAME="main"
          
          echo "Creating/updating Amplify app..."
          echo "App name: $AMPLIFY_APP_NAME"
          
          # First, try to find existing app
          AMPLIFY_APP_ID=$(aws amplify list-apps \
            --query "apps[?name=='$AMPLIFY_APP_NAME'].appId" \
            --output text)
          
          if [ -z "$AMPLIFY_APP_ID" ] || [ "$AMPLIFY_APP_ID" = "None" ]; then
            echo "No existing app found, creating new Amplify app..."
            AMPLIFY_APP_ID=$(aws amplify create-app \
              --name "$AMPLIFY_APP_NAME" \
              --platform WEB \
              --environment-variables "REACT_APP_API_BASE_URL=$API_URL,REACT_APP_CHAT_ENDPOINT=$API_URL,REACT_APP_HEALTH_ENDPOINT=$API_URL" \
              --query 'app.appId' \
              --output text)
          else
            echo "Found existing app with ID: $AMPLIFY_APP_ID"
            # Update environment variables for existing app
            aws amplify update-app \
              --app-id "$AMPLIFY_APP_ID" \
              --environment-variables "REACT_APP_API_BASE_URL=$API_URL,REACT_APP_CHAT_ENDPOINT=$API_URL,REACT_APP_HEALTH_ENDPOINT=$API_URL"
          fi
          
          echo "Final Amplify App ID: $AMPLIFY_APP_ID"
          
          if [ -z "$AMPLIFY_APP_ID" ] || [ "$AMPLIFY_APP_ID" = "None" ]; then
            echo "‚ùå ERROR: Failed to create or find Amplify app"
            exit 1
          fi
          
          # Create branch if it doesn't exist
          aws amplify create-branch \
            --app-id "$AMPLIFY_APP_ID" \
            --branch-name "$AMPLIFY_BRANCH_NAME" \
            --stage PRODUCTION \
            --environment-variables "REACT_APP_API_BASE_URL=$API_URL,REACT_APP_CHAT_ENDPOINT=$API_URL,REACT_APP_HEALTH_ENDPOINT=$API_URL" \
            --no-cli-pager 2>/dev/null || echo "Branch already exists"
          
          # Update AmplifyDeployer Lambda with app details
          AMPLIFY_DEPLOYER_FUNCTION_NAME=$(cat outputs.json | jq -r '.AmericasBloodCentersBedrockStack.AmplifyDeployerFunctionName // empty')
          
          if [ -n "$AMPLIFY_DEPLOYER_FUNCTION_NAME" ] && [ "$AMPLIFY_DEPLOYER_FUNCTION_NAME" != "empty" ]; then
            aws lambda update-function-configuration \
              --function-name "$AMPLIFY_DEPLOYER_FUNCTION_NAME" \
              --environment "Variables={AMPLIFY_APP_ID=$AMPLIFY_APP_ID,AMPLIFY_BRANCH_NAME=$AMPLIFY_BRANCH_NAME}" \
              --region "$CDK_DEFAULT_REGION"
            echo "‚úì AmplifyDeployer Lambda updated"
          fi
          
          echo ""
          echo "=== Frontend Build and Automated Deployment ==="
          
          # Check if Frontend directory exists
          if [ -d "../Frontend" ]; then
            cd ../Frontend
            
            # Set up environment variables
            export PUBLIC_URL=""
            export GENERATE_SOURCEMAP=false
            echo "REACT_APP_API_BASE_URL=$API_URL" > .env.production
            echo "REACT_APP_CHAT_ENDPOINT=$API_URL" >> .env.production
            echo "REACT_APP_HEALTH_ENDPOINT=$API_URL" >> .env.production
            echo "PUBLIC_URL=" >> .env.production
            echo "GENERATE_SOURCEMAP=false" >> .env.production
            
            # Clean and build
            rm -rf build/ node_modules/.cache/
            npm install
            npm run build
            
            if [ ! -f "build/index.html" ]; then
              echo "‚ùå ERROR: Build failed - index.html not found!"
              exit 1
            fi
            
            if grep -q "%PUBLIC_URL%" build/index.html; then
              echo "‚ùå ERROR: Build incomplete - %PUBLIC_URL% not replaced!"
              exit 1
            fi
            
            echo "‚úÖ Build successful"
            
            # Create deployment package
            cd build
            zip -r ../build.zip . -x "*.DS_Store" "*.map"
            cd ..
            
            # Upload to builds bucket
            BUILD_KEY="builds/build-$(date +%s).zip"
            BUILDS_BUCKET=$(cat ../Backend/outputs.json | jq -r '.AmericasBloodCentersBedrockStack.BuildsBucketName // empty')
            
            if [ -n "$BUILDS_BUCKET" ] && [ "$BUILDS_BUCKET" != "empty" ]; then
              aws s3 cp build.zip "s3://$BUILDS_BUCKET/$BUILD_KEY"
              echo "‚úÖ Build artifact uploaded to S3: $BUILD_KEY"
              
              # Trigger deployment via AmplifyDeployer Lambda if available
              if [ -n "$AMPLIFY_DEPLOYER_FUNCTION_NAME" ] && [ "$AMPLIFY_DEPLOYER_FUNCTION_NAME" != "empty" ]; then
                DEPLOY_PAYLOAD='{"bucket":"'$BUILDS_BUCKET'","key":"'$BUILD_KEY'"}'
                aws lambda invoke \
                  --function-name "$AMPLIFY_DEPLOYER_FUNCTION_NAME" \
                  --payload "$DEPLOY_PAYLOAD" \
                  --cli-binary-format raw-in-base64-out \
                  --region "$CDK_DEFAULT_REGION" \
                  /tmp/deploy_response.json
                echo "‚úÖ Deployment triggered via Lambda"
              else
                # Fallback: Direct Amplify deployment
                aws amplify start-deployment \
                  --app-id "$AMPLIFY_APP_ID" \
                  --branch-name "$AMPLIFY_BRANCH_NAME" \
                  --source-url "s3://$BUILDS_BUCKET/$BUILD_KEY" \
                  --region "$CDK_DEFAULT_REGION"
                echo "‚úÖ Direct Amplify deployment started"
              fi
              
              echo "üöÄ Automated deployment initiated"
              echo "üì± Amplify App URL: https://$AMPLIFY_BRANCH_NAME.$AMPLIFY_APP_ID.amplifyapp.com"
            else
              echo "‚ö†Ô∏è Builds bucket not found, skipping S3 upload"
            fi
            
            cd ../Backend
          else
            echo "‚ö†Ô∏è Frontend directory not found, skipping frontend deployment"
          fi
          
          echo ""
          echo "=== Monitoring Data Source Ingestion ==="
          
          # Monitor S3 ingestion job
          if [ -n "$S3_INGESTION_JOB_ID" ] && [ "$S3_INGESTION_JOB_ID" != "None" ]; then
            echo "Monitoring S3 ingestion job: $S3_INGESTION_JOB_ID"
            S3_JOB_STATUS="IN_PROGRESS"
            WAIT_TIME=0
            MAX_WAIT=600  # 10 minutes max wait
            
            while [ "$S3_JOB_STATUS" = "IN_PROGRESS" ] && [ $WAIT_TIME -lt $MAX_WAIT ]; do
              sleep 30
              WAIT_TIME=$((WAIT_TIME + 30))
              
              S3_JOB_STATUS=$(aws bedrock-agent get-ingestion-job \
                --knowledge-base-id "$KB_ID" \
                --data-source-id "$S3_DS_ID" \
                --ingestion-job-id "$S3_INGESTION_JOB_ID" \
                --region "$CDK_DEFAULT_REGION" \
                --query 'ingestionJob.status' --output text)
              
              echo "S3 ingestion status: $S3_JOB_STATUS (waited ${WAIT_TIME}s)"
            done
            
            if [ "$S3_JOB_STATUS" = "COMPLETE" ]; then
              echo "‚úÖ S3 ingestion completed successfully"
            else
              echo "‚ö†Ô∏è S3 ingestion status: $S3_JOB_STATUS (continuing anyway)"
            fi
          else
            echo "‚ö†Ô∏è No S3 ingestion job to monitor"
          fi
          
          # Monitor Web Crawler ingestion job (shorter wait since it takes longer)
          if [ -n "$WEB_INGESTION_JOB_ID" ] && [ "$WEB_INGESTION_JOB_ID" != "None" ]; then
            echo "Checking Web Crawler ingestion job: $WEB_INGESTION_JOB_ID"
            WEB_JOB_STATUS=$(aws bedrock-agent get-ingestion-job \
              --knowledge-base-id "$KB_ID" \
              --data-source-id "$WEB_DS_ID" \
              --ingestion-job-id "$WEB_INGESTION_JOB_ID" \
              --region "$CDK_DEFAULT_REGION" \
              --query 'ingestionJob.status' --output text)
            
            echo "Web Crawler ingestion status: $WEB_JOB_STATUS"
            
            if [ "$WEB_JOB_STATUS" = "IN_PROGRESS" ]; then
              echo "‚è≥ Web crawling is in progress (this can take 10-20 minutes)"
              echo "üìä You can monitor progress in the Bedrock console"
            elif [ "$WEB_JOB_STATUS" = "COMPLETE" ]; then
              echo "‚úÖ Web crawling completed successfully"
            else
              echo "‚ö†Ô∏è Web crawling status: $WEB_JOB_STATUS"
            fi
          else
            echo "‚ö†Ô∏è No Web ingestion job to monitor"
          fi
          
          echo ""
          echo "========================================="
          echo "‚úÖ Deployment Complete!"
          echo "========================================="
          echo "üß† Knowledge Base ID: $KB_ID"
          echo "üìÑ S3 Data Source ID: $S3_DS_ID (PDFs with Bedrock Data Automation)"
          echo "üåê Web Crawler Data Source ID: $WEB_DS_ID (Websites)"
          echo "üìÅ Documents Bucket: $DOCUMENTS_BUCKET"
          echo "üîç OpenSearch Endpoint: $OPENSEARCH_ENDPOINT"
          echo "üìä S3 Ingestion Job ID: $S3_INGESTION_JOB_ID"
          echo "üï∑Ô∏è Web Ingestion Job ID: $WEB_INGESTION_JOB_ID"
          echo "üåê API Gateway URL: $API_URL"
          echo "üì± Frontend URL: https://$AMPLIFY_BRANCH_NAME.$AMPLIFY_APP_ID.amplifyapp.com"
          echo "üöÄ Deployment Type: Automated via Amplify (no GitHub required)"
          echo "========================================="
          echo ""
          echo "========================================="
          echo "üöÄ Next Steps"
          echo "========================================="
          echo "1Ô∏è‚É£ Wait 5-10 minutes for ingestion jobs to complete"
          echo ""
          echo "2Ô∏è‚É£ Test the API:"
          echo "   curl -X POST $API_URL \\"
          echo "     -H 'Content-Type: application/json' \\"
          echo "     -d '{\"message\":\"How many people donate blood?\",\"language\":\"en\"}'"
          echo ""
          echo "3Ô∏è‚É£ Test Spanish support:"
          echo "   curl -X POST $API_URL \\"
          echo "     -H 'Content-Type: application/json' \\"
          echo "     -d '{\"message\":\"¬øCu√°ntas personas donan sangre?\",\"language\":\"es\"}'"
          echo ""
          echo "4Ô∏è‚É£ Monitor ingestion jobs:"
          echo "   # S3 Data Source (PDFs):"
          echo "   aws bedrock-agent get-ingestion-job \\"
          echo "     --knowledge-base-id $KB_ID \\"
          echo "     --data-source-id $S3_DS_ID \\"
          echo "     --ingestion-job-id $S3_INGESTION_JOB_ID \\"
          echo "     --region $CDK_DEFAULT_REGION"
          echo ""
          echo "   # Web Crawler Data Source:"
          echo "   aws bedrock-agent get-ingestion-job \\"
          echo "     --knowledge-base-id $KB_ID \\"
          echo "     --data-source-id $WEB_DS_ID \\"
          echo "     --ingestion-job-id $WEB_INGESTION_JOB_ID \\"
          echo "     --region $CDK_DEFAULT_REGION"
          echo ""
          echo "5Ô∏è‚É£ Add more documents:"
          echo "   # Add PDFs (uses Bedrock Data Automation parser):"
          echo "   aws s3 cp document.pdf s3://$DOCUMENTS_BUCKET/pdfs/"
          echo "   aws bedrock-agent start-ingestion-job \\"
          echo "     --knowledge-base-id $KB_ID \\"
          echo "     --data-source-id $S3_DS_ID \\"
          echo "     --region $CDK_DEFAULT_REGION"
          echo ""
          echo "   # Re-crawl websites (automatic parsing):"
          echo "   aws bedrock-agent start-ingestion-job \\"
          echo "     --knowledge-base-id $KB_ID \\"
          echo "     --data-source-id $WEB_DS_ID \\"
          echo "     --region $CDK_DEFAULT_REGION"
          echo ""
          echo "üìä Monitor resources:"
          echo "  ‚Ä¢ Knowledge Base: https://console.aws.amazon.com/bedrock/home?region=${CDK_DEFAULT_REGION}#/knowledge-bases"
          echo "  ‚Ä¢ OpenSearch: https://console.aws.amazon.com/aos/home?region=${CDK_DEFAULT_REGION}#opensearch/collections"
          echo "  ‚Ä¢ S3 Bucket: https://s3.console.aws.amazon.com/s3/buckets/$DOCUMENTS_BUCKET"
          echo ""
          echo "üí∞ Estimated monthly cost: \$8-20 (vs \$20+ for Q Business)"
          echo "üéØ Features: Dual data sources, bilingual support, automatic web crawling, markdown support"
          echo "üöÄ Frontend: Automatically deployed to Amplify (no GitHub required)"
          echo ""
        fi

  post_build:
    commands:
      - echo "========================================="
      - echo "üéâ Deployment Complete"
      - echo "========================================="
      - |
        if [ "$ACTION" = "deploy" ]; then
          echo "‚úÖ Bedrock chatbot deployed successfully"
          echo "üìä Check CloudWatch Logs for Lambda execution details"
          echo "üß† Test Knowledge Base retrieval via the chat API"
          echo "üåê Monitor ingestion jobs in Bedrock console"
        else
          echo "‚úÖ Stack destroyed successfully"
        fi

artifacts:
  files:
    - "**/*"
  base-directory: "Backend/cdk.out"