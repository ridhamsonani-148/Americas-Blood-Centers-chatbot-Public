version: 0.2

phases:
  install:
    runtime-versions:
      nodejs: 20
      python: 3.11
    commands:
      - echo "Installing AWS CDK CLI..."
      - npm install -g aws-cdk@latest
      - cd Backend
      - npm install

  pre_build:
    commands:
      - echo "=== Building TypeScript ==="
      - npm run build
      - echo "âœ… TypeScript build completed"
      - echo "=== Installing Lambda dependencies ==="
      - cd lambda && pip install -r requirements.txt -t . && cd ..
      - echo "âœ… Lambda dependencies installed"
      - echo "=== Bootstrapping CDK Environment ==="
      - |
        # Set default PROJECT_NAME for bootstrap if not provided
        if [ -z "$PROJECT_NAME" ]; then
          PROJECT_NAME="abc"
        fi
        cdk bootstrap --require-approval never \
          --context projectName="$PROJECT_NAME" \
          --context modelId="${MODEL_ID:-anthropic.claude-3-haiku-20240307-v1:0}" \
          --context embeddingModelId="${EMBEDDING_MODEL_ID:-amazon.titan-embed-text-v1}"
      - echo "âœ… CDK Bootstrap completed"

  build:
    commands:
      - |
        if [ "$ACTION" = "destroy" ]; then
          echo "=== Destroying Stack ==="
          cdk destroy AmericasBloodCentersBedrockStack --force \
            --context projectName="$PROJECT_NAME" \
            --context modelId="$MODEL_ID" \
            --context embeddingModelId="$EMBEDDING_MODEL_ID"
          echo "âœ… Stack destroyed successfully"
        else
          echo "========================================="
          echo "Deploying America's Blood Centers Bedrock Chatbot"
          echo "========================================="
          
          echo "=== PHASE 1: CDK Infrastructure Deployment ==="
          
          # Set default PROJECT_NAME if not provided
          if [ -z "$PROJECT_NAME" ]; then
            PROJECT_NAME="abc"
            echo "ðŸ” Using default PROJECT_NAME: $PROJECT_NAME"
          else
            echo "ðŸ” Using provided PROJECT_NAME: $PROJECT_NAME"
          fi
          
          cdk deploy AmericasBloodCentersBedrockStack --require-approval never \
            --context projectName="$PROJECT_NAME" \
            --context modelId="$MODEL_ID" \
            --context embeddingModelId="$EMBEDDING_MODEL_ID" \
            --outputs-file outputs.json
          
          if [ $? -ne 0 ]; then
            echo "âŒ ERROR: CDK deployment failed"
            exit 1
          fi
          
          echo "âœ… CDK deployment successful"
          
          echo "=== PHASE 2: Extracting CDK Outputs ==="
          STACK_NAME="AmericasBloodCentersBedrockStack"
          DOCUMENTS_BUCKET=$(cat outputs.json | jq -r '.AmericasBloodCentersBedrockStack.DocumentsBucketName // empty')
          KB_ROLE_ARN=$(cat outputs.json | jq -r '.AmericasBloodCentersBedrockStack.KnowledgeBaseRoleArn // empty')
          CHAT_LAMBDA_NAME=$(cat outputs.json | jq -r '.AmericasBloodCentersBedrockStack.ChatLambdaFunctionName // empty')
          DATA_INGESTION_LAMBDA_NAME=$(cat outputs.json | jq -r '.AmericasBloodCentersBedrockStack.DataIngestionFunctionName // empty')
          API_URL=$(cat outputs.json | jq -r '.AmericasBloodCentersBedrockStack.ApiGatewayUrl // empty')
          AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
          
          if [ -z "$DOCUMENTS_BUCKET" ] || [ -z "$KB_ROLE_ARN" ] || [ -z "$CHAT_LAMBDA_NAME" ]; then
            echo "âŒ ERROR: Failed to extract required outputs from CDK deployment"
            echo "Documents Bucket: $DOCUMENTS_BUCKET"
            echo "KB Role ARN: $KB_ROLE_ARN"
            echo "Chat Lambda: $CHAT_LAMBDA_NAME"
            exit 1
          fi
          
          echo "âœ… Outputs extracted successfully"
          echo "  ðŸ“ Documents Bucket: $DOCUMENTS_BUCKET"
          echo "  ðŸ” KB Role ARN: $KB_ROLE_ARN"
          echo "  ðŸ’¬ Chat Lambda: $CHAT_LAMBDA_NAME"
          echo "  ðŸ“Š Data Ingestion Lambda: $DATA_INGESTION_LAMBDA_NAME"
          echo "  ðŸŒ API Gateway URL: $API_URL"
          echo ""
          echo "========================================="
          echo "=== PHASE 3: Knowledge Base Resources ==="
          echo "========================================="
          
          # Step 3: Create or reuse OpenSearch Serverless Collection
          echo "Step 3: Creating or reusing OpenSearch Serverless collection..."
          
          # Use a consistent collection name (no timestamp for reusability)
          COLLECTION_NAME="${PROJECT_NAME}-kb-collection"
          echo "ðŸ” Debug: PROJECT_NAME='$PROJECT_NAME'"
          echo "ðŸ” Debug: COLLECTION_NAME='$COLLECTION_NAME'"
          echo "ðŸ” Debug: Collection name length: ${#COLLECTION_NAME}"
          
          # Check if collection already exists
          EXISTING_COLLECTION_ARN=$(aws opensearchserverless list-collections \
            --region "$CDK_DEFAULT_REGION" \
            --query "collectionSummaries[?name=='$COLLECTION_NAME'].arn | [0]" \
            --output text 2>/dev/null || echo "")
          
          if [ -n "$EXISTING_COLLECTION_ARN" ] && [ "$EXISTING_COLLECTION_ARN" != "None" ] && [ "$EXISTING_COLLECTION_ARN" != "null" ]; then
            echo "âœ“ Found existing OpenSearch collection: $EXISTING_COLLECTION_ARN"
            COLLECTION_ARN="$EXISTING_COLLECTION_ARN"
            
            # Check if collection is active
            COLLECTION_STATUS=$(aws opensearchserverless list-collections \
              --region "$CDK_DEFAULT_REGION" \
              --query "collectionSummaries[?name=='$COLLECTION_NAME'].status | [0]" \
              --output text)
            
            if [ "$COLLECTION_STATUS" = "ACTIVE" ]; then
              echo "âœ“ Collection is already active"
            else
              echo "âš  Collection exists but status is: $COLLECTION_STATUS"
              echo "Waiting for collection to become active..."
            fi
          else
            echo "Creating new OpenSearch Serverless collection..."
            
            # Create or update network security policy
            echo "Creating/updating network security policy..."
            if aws opensearchserverless get-security-policy --name "${PROJECT_NAME}-network-policy" --type network --region "$CDK_DEFAULT_REGION" >/dev/null 2>&1; then
              echo "âœ“ Network security policy already exists, updating if needed..."
              # Get current policy version
              NETWORK_POLICY_VERSION=$(aws opensearchserverless get-security-policy \
                --name "${PROJECT_NAME}-network-policy" \
                --type network \
                --region "$CDK_DEFAULT_REGION" \
                --query 'securityPolicyDetail.policyVersion' \
                --output text)
              # Update the policy to include the current collection
              aws opensearchserverless update-security-policy \
                --name "${PROJECT_NAME}-network-policy" \
                --type network \
                --policy-version "$NETWORK_POLICY_VERSION" \
                --policy '[{"Rules":[{"Resource":["collection/'$COLLECTION_NAME'"],"ResourceType":"collection"}],"AllowFromPublic":true}]' \
                --region "$CDK_DEFAULT_REGION" && echo "âœ“ Network security policy updated"
            else
              aws opensearchserverless create-security-policy \
                --name "${PROJECT_NAME}-network-policy" \
                --type network \
                --policy '[{"Rules":[{"Resource":["collection/'$COLLECTION_NAME'"],"ResourceType":"collection"}],"AllowFromPublic":true}]' \
                --region "$CDK_DEFAULT_REGION" && echo "âœ“ Network security policy created"
            fi
            
            # Create or update encryption security policy
            echo "Creating/updating encryption security policy..."
            if aws opensearchserverless get-security-policy --name "${PROJECT_NAME}-encryption-policy" --type encryption --region "$CDK_DEFAULT_REGION" >/dev/null 2>&1; then
              echo "âœ“ Encryption security policy already exists, updating if needed..."
              # Get current policy version
              ENCRYPTION_POLICY_VERSION=$(aws opensearchserverless get-security-policy \
                --name "${PROJECT_NAME}-encryption-policy" \
                --type encryption \
                --region "$CDK_DEFAULT_REGION" \
                --query 'securityPolicyDetail.policyVersion' \
                --output text)
              aws opensearchserverless update-security-policy \
                --name "${PROJECT_NAME}-encryption-policy" \
                --type encryption \
                --policy-version "$ENCRYPTION_POLICY_VERSION" \
                --policy '{"Rules":[{"Resource":["collection/'$COLLECTION_NAME'"],"ResourceType":"collection"}],"AWSOwnedKey":true}' \
                --region "$CDK_DEFAULT_REGION" && echo "âœ“ Encryption security policy updated"
            else
              aws opensearchserverless create-security-policy \
                --name "${PROJECT_NAME}-encryption-policy" \
                --type encryption \
                --policy '{"Rules":[{"Resource":["collection/'$COLLECTION_NAME'"],"ResourceType":"collection"}],"AWSOwnedKey":true}' \
                --region "$CDK_DEFAULT_REGION" && echo "âœ“ Encryption security policy created"
            fi
            
            # Create or update data access policy
            echo "Creating/updating data access policy..."
            # Get CodeBuild role ARN for access policy
            CODEBUILD_ROLE_ARN=$(aws sts get-caller-identity --query Arn --output text | sed 's/:user\//:role\//' | sed 's/\/[^\/]*$/\/codebuild-service-role/')
            echo "ðŸ” Debug: CodeBuild role ARN: $CODEBUILD_ROLE_ARN"
            
            if aws opensearchserverless get-access-policy --name "${PROJECT_NAME}-data-access-policy" --type data --region "$CDK_DEFAULT_REGION" >/dev/null 2>&1; then
              echo "âœ“ Data access policy already exists, will update with KB role later"
            else
              # Create new data access policy
              aws opensearchserverless create-access-policy \
                --name "${PROJECT_NAME}-data-access-policy" \
                --type data \
                --policy '[{"Rules":[{"Resource":["collection/'$COLLECTION_NAME'"],"Permission":["aoss:CreateCollectionItems","aoss:DeleteCollectionItems","aoss:UpdateCollectionItems","aoss:DescribeCollectionItems"],"ResourceType":"collection"},{"Resource":["index/'$COLLECTION_NAME'/*"],"Permission":["aoss:CreateIndex","aoss:DeleteIndex","aoss:UpdateIndex","aoss:DescribeIndex","aoss:ReadDocument","aoss:WriteDocument"],"ResourceType":"index"}],"Principal":["arn:aws:iam::'$AWS_ACCOUNT_ID':root","'$CODEBUILD_ROLE_ARN'"]}]' \
                --region "$CDK_DEFAULT_REGION" && echo "âœ“ Data access policy created (will update with KB role later)"
            fi
            
            # Wait for policies to propagate
            echo "Waiting for policies to propagate..."
            sleep 15
          fi
          
          # Create collection only if it doesn't exist
          if [ -z "$EXISTING_COLLECTION_ARN" ] || [ "$EXISTING_COLLECTION_ARN" = "None" ] || [ "$EXISTING_COLLECTION_ARN" = "null" ]; then
            echo "ðŸ” Debug: EXISTING_COLLECTION_ARN is empty/None/null, creating new collection"
            echo "ðŸ” Debug: EXISTING_COLLECTION_ARN value: '$EXISTING_COLLECTION_ARN'"
            echo "Creating OpenSearch Serverless collection..."
            COLLECTION_CREATE_OUTPUT=$(aws opensearchserverless create-collection \
              --name "$COLLECTION_NAME" \
              --type VECTORSEARCH \
              --description "Vector collection for America's Blood Centers knowledge base" \
              --region "$CDK_DEFAULT_REGION" 2>&1)
            
            COLLECTION_CREATE_EXIT_CODE=$?
            echo "ðŸ” Debug: Collection creation exit code: $COLLECTION_CREATE_EXIT_CODE"
            echo "ðŸ” Debug: Collection creation output: $COLLECTION_CREATE_OUTPUT"
            
            if [ $COLLECTION_CREATE_EXIT_CODE -eq 0 ]; then
              COLLECTION_ARN=$(echo "$COLLECTION_CREATE_OUTPUT" | jq -r '.createCollectionDetail.arn // empty')
              echo "âœ“ Collection creation initiated: $COLLECTION_ARN"
            else
              echo "âœ— Failed to create OpenSearch collection"
              echo "Error: $COLLECTION_CREATE_OUTPUT"
              exit 1
            fi
          else
            echo "ðŸ” Debug: Using existing collection: $EXISTING_COLLECTION_ARN"
            COLLECTION_ARN="$EXISTING_COLLECTION_ARN"
          fi
          
          # Step 4: Wait for collection to be active
          echo "Step 4: Waiting for collection to be active..."
          MAX_WAIT=900  # Increased to 15 minutes
          ELAPSED=0
          while [ $ELAPSED -lt $MAX_WAIT ]; do
            STATUS=$(aws opensearchserverless list-collections \
              --region "$CDK_DEFAULT_REGION" \
              --query "collectionSummaries[?name=='$COLLECTION_NAME'].status | [0]" \
              --output text 2>/dev/null || echo "ERROR")
            
            echo "  Collection status: $STATUS (waited ${ELAPSED}s/${MAX_WAIT}s)"
            
            if [ "$STATUS" = "ACTIVE" ]; then
              echo "âœ“ Collection is active"
              # Get the final collection ARN
              COLLECTION_ARN=$(aws opensearchserverless list-collections \
                --region "$CDK_DEFAULT_REGION" \
                --query "collectionSummaries[?name=='$COLLECTION_NAME'].arn | [0]" \
                --output text)
              echo "  Final Collection ARN: $COLLECTION_ARN"
              break
            elif [ "$STATUS" = "FAILED" ]; then
              echo "âœ— Collection creation failed"
              # Get collection details for debugging
              aws opensearchserverless list-collections \
                --region "$CDK_DEFAULT_REGION" \
                --query "collectionSummaries[?name=='$COLLECTION_NAME']" \
                --output json
              exit 1
            elif [ "$STATUS" = "ERROR" ] || [ -z "$STATUS" ] || [ "$STATUS" = "None" ] || [ "$STATUS" = "null" ]; then
              echo "âœ— Failed to get collection status or collection doesn't exist"
              echo "ðŸ” Debug: Checking if collection was actually created..."
              aws opensearchserverless list-collections \
                --region "$CDK_DEFAULT_REGION" \
                --query "collectionSummaries[?name=='$COLLECTION_NAME']" \
                --output json
              
              # If no collection found after some time, exit
              if [ $ELAPSED -gt 300 ]; then
                echo "âœ— Collection not found after 5 minutes, something went wrong"
                exit 1
              fi
            fi
            
            sleep 30
            ELAPSED=$((ELAPSED + 30))
          done
          
          if [ $ELAPSED -ge $MAX_WAIT ]; then
            echo "âœ— Timeout waiting for collection to be active after ${MAX_WAIT} seconds"
            echo "Current status: $STATUS"
            exit 1
          fi
          
          # Step 4.5: Update data access policy with Knowledge Base role
          echo "Step 4.5: Adding Knowledge Base role to OpenSearch data access policy..."
          
          # Extract just the role name from the full ARN
          KB_ROLE_NAME=$(echo "$KB_ROLE_ARN" | sed 's/.*role\///')
          echo "ðŸ” Debug: KB Role Name: $KB_ROLE_NAME"
          echo "ðŸ” Debug: KB Role ARN: $KB_ROLE_ARN"
          
          # Get current policy version
          echo "Getting current data access policy version..."
          POLICY_VERSION=$(aws opensearchserverless get-access-policy \
            --name "${PROJECT_NAME}-data-access-policy" \
            --type data \
            --region "$CDK_DEFAULT_REGION" \
            --query 'accessPolicyDetail.policyVersion' \
            --output text)
          
          echo "ðŸ” Debug: Current policy version: $POLICY_VERSION"
          
          # Get CodeBuild role ARN for the policy update
          CODEBUILD_ROLE_ARN=$(aws sts get-caller-identity --query Arn --output text | sed 's/:user\//:role\//' | sed 's/\/[^\/]*$/\/codebuild-service-role/')
          
          # Update the data access policy to include the Knowledge Base role
          echo "Updating data access policy to include Knowledge Base role..."
          aws opensearchserverless update-access-policy \
            --name "${PROJECT_NAME}-data-access-policy" \
            --type data \
            --policy-version "$POLICY_VERSION" \
            --policy '[{"Rules":[{"Resource":["collection/'$COLLECTION_NAME'"],"Permission":["aoss:CreateCollectionItems","aoss:DeleteCollectionItems","aoss:UpdateCollectionItems","aoss:DescribeCollectionItems"],"ResourceType":"collection"},{"Resource":["index/'$COLLECTION_NAME'/*"],"Permission":["aoss:CreateIndex","aoss:DeleteIndex","aoss:UpdateIndex","aoss:DescribeIndex","aoss:ReadDocument","aoss:WriteDocument"],"ResourceType":"index"}],"Principal":["arn:aws:iam::'$AWS_ACCOUNT_ID':root","'$CODEBUILD_ROLE_ARN'","'$KB_ROLE_ARN'"]}]' \
            --region "$CDK_DEFAULT_REGION" && echo "âœ“ Data access policy updated with Knowledge Base role"
          
          # Wait for policy update to propagate
          echo "Waiting for policy update to propagate..."
          sleep 10
          
          # Step 4.6: Create the required vector index manually (if it doesn't exist)
          echo "Step 4.6: Creating vector index in OpenSearch collection..."
          
          # Get the collection ID from the ARN
          COLLECTION_ID=$(echo "$COLLECTION_ARN" | sed 's/.*collection\///')
          echo "ðŸ” Debug: Collection ID extracted: $COLLECTION_ID"
          
          # Check if index already exists
          echo "Checking if vector index already exists..."
          INDEX_EXISTS=$(aws opensearchserverless list-indexes \
            --region "$CDK_DEFAULT_REGION" \
            --collection-id "$COLLECTION_ID" \
            --query 'indexDetails[?name==`bedrock-knowledge-base-default-index`].name | [0]' \
            --output text 2>/dev/null || echo "")
          
          if [ -n "$INDEX_EXISTS" ] && [ "$INDEX_EXISTS" != "None" ] && [ "$INDEX_EXISTS" != "null" ]; then
            echo "âœ“ Vector index already exists: bedrock-knowledge-base-default-index"
          else
            # Create the index using AWS CLI (now that we have proper permissions)
            echo "Creating bedrock-knowledge-base-default-index..."
            echo "ðŸ” Debug: Using collection ID: $COLLECTION_ID"
            echo "ðŸ” Debug: Region: $CDK_DEFAULT_REGION"
            
            # Try to create the index and capture both stdout and stderr
            set +e  # Temporarily disable exit on error
            INDEX_CREATION_OUTPUT=$(aws opensearchserverless create-index \
              --region "$CDK_DEFAULT_REGION" \
              --id "$COLLECTION_ID" \
              --index-name "bedrock-knowledge-base-default-index" \
              --index-schema '{
                "settings": {
                  "index.knn": true
                },
                "mappings": {
                  "properties": {
                    "bedrock-knowledge-base-default-vector": {
                      "type": "knn_vector",
                      "dimension": 1536,
                      "method": {
                        "name": "hnsw",
                        "engine": "faiss",
                        "parameters": {
                          "ef_construction": 512,
                          "m": 16
                        }
                      }
                    },
                    "AMAZON_BEDROCK_TEXT_CHUNK": {
                      "type": "text"
                    },
                    "AMAZON_BEDROCK_METADATA": {
                      "type": "text"
                    }
                  }
                }
              }' 2>&1)
            INDEX_EXIT_CODE=$?
            set -e  # Re-enable exit on error
            
            echo "ðŸ” Debug: Index creation exit code: $INDEX_EXIT_CODE"
            echo "ðŸ” Debug: Index creation output: $INDEX_CREATION_OUTPUT"
            
            if echo "$INDEX_CREATION_OUTPUT" | grep -q "indexDetail"; then
              echo "âœ“ Vector index created successfully"
              echo "Index details: $INDEX_CREATION_OUTPUT"
            elif echo "$INDEX_CREATION_OUTPUT" | grep -q "already exists\|ResourceAlreadyExistsException"; then
              echo "âœ“ Index already exists, continuing..."
            else
              echo "âš  Index creation response: $INDEX_CREATION_OUTPUT"
              echo "âš  Index creation may have failed, but continuing..."
            fi
          fi
          
          # Wait for index to be ready
          echo "Waiting for index to be ready..."
          sleep 15
          
          echo "âœ“ Collection and index are ready for Knowledge Base creation"
          
          # Step 5: Create Knowledge Base with Supplemental Data Storage
          echo "Step 5: Creating Bedrock Knowledge Base with supplemental storage..."
          
          # Get supplemental bucket from CDK stack outputs
          SUPPLEMENTAL_BUCKET=$(aws cloudformation describe-stacks \
            --stack-name "$STACK_NAME" \
            --query "Stacks[0].Outputs[?OutputKey=='SupplementalBucketName'].OutputValue | [0]" \
            --output text)
          
          if [ -z "$SUPPLEMENTAL_BUCKET" ] || [ "$SUPPLEMENTAL_BUCKET" = "None" ]; then
            echo "âœ— Supplemental bucket not found in CDK stack outputs"
            exit 1
          else
            echo "âœ“ Using CDK-created supplemental bucket: $SUPPLEMENTAL_BUCKET"
          fi
          
          # Permissions already handled by CDK
          echo "âœ“ Supplemental bucket permissions configured by CDK"
          
          KB_ID=$(aws bedrock-agent list-knowledge-bases \
            --region "$CDK_DEFAULT_REGION" \
            --query "knowledgeBaseSummaries[?name=='${PROJECT_NAME}-knowledge-base'].knowledgeBaseId | [0]" \
            --output text 2>/dev/null || echo "")
          
          if [ -n "$KB_ID" ] && [ "$KB_ID" != "None" ] && [ "$KB_ID" != "null" ]; then
            echo "âœ“ Found existing Knowledge Base: $KB_ID"
            echo "âš  Deleting existing Knowledge Base to recreate with supplemental storage..."
            aws bedrock-agent delete-knowledge-base \
              --knowledge-base-id "$KB_ID" \
              --region "$CDK_DEFAULT_REGION"
            
            # Wait for deletion to complete
            echo "Waiting for Knowledge Base deletion..."
            sleep 30
          fi
          
          echo "Creating new Bedrock Knowledge Base with supplemental storage..."
          echo "ðŸ” Debug: KB Role ARN: $KB_ROLE_ARN"
          echo "ðŸ” Debug: Collection ARN: $COLLECTION_ARN"
          echo "ðŸ” Debug: Supplemental Bucket: $SUPPLEMENTAL_BUCKET"
          echo "ðŸ” Debug: Region: $CDK_DEFAULT_REGION"
          echo "ðŸ” Debug: Embedding Model: $EMBEDDING_MODEL_ID"
          
          # Verify bucket access and ownership
          echo "Verifying supplemental bucket access..."
          BUCKET_OWNER=$(aws s3api get-bucket-location --bucket "$SUPPLEMENTAL_BUCKET" --query 'LocationConstraint' --output text 2>/dev/null || echo "ERROR")
          if [ "$BUCKET_OWNER" != "ERROR" ]; then
            echo "âœ“ Bucket access verified"
            echo "ðŸ” Debug: Bucket region: $BUCKET_OWNER"
          else
            echo "âš  Could not verify bucket access"
          fi
          
          # Test if the role can access the bucket (simulate what Bedrock will do)
          echo "Testing bucket permissions..."
          aws s3 ls "s3://$SUPPLEMENTAL_BUCKET" >/dev/null 2>&1 && echo "âœ“ Bucket listing successful" || echo "âš  Bucket listing failed"
          
          # Try to create Knowledge Base with detailed error capture
          set +e  # Temporarily disable exit on error
          echo "Executing create-knowledge-base command..."
          KB_CREATE_OUTPUT=$(aws bedrock-agent create-knowledge-base \
            --name "${PROJECT_NAME}-knowledge-base" \
            --description "Knowledge base for America's Blood Centers chatbot with BDA parsing" \
            --role-arn "$KB_ROLE_ARN" \
            --knowledge-base-configuration '{
              "type": "VECTOR",
              "vectorKnowledgeBaseConfiguration": {
                "embeddingModelArn": "arn:aws:bedrock:'"$CDK_DEFAULT_REGION"'::foundation-model/'"$EMBEDDING_MODEL_ID"'",
                "supplementalDataStorageConfiguration": {
                  "storageLocations": [
                    {
                      "type": "S3",
                      "s3Location": {
                        "uri": "s3://'"$SUPPLEMENTAL_BUCKET"'"
                      }
                    }
                  ]
                }
              }
            }' \
            --storage-configuration '{
              "type": "OPENSEARCH_SERVERLESS",
              "opensearchServerlessConfiguration": {
                "collectionArn": "'"$COLLECTION_ARN"'",
                "vectorIndexName": "bedrock-knowledge-base-default-index",
                "fieldMapping": {
                  "vectorField": "bedrock-knowledge-base-default-vector",
                  "textField": "AMAZON_BEDROCK_TEXT_CHUNK",
                  "metadataField": "AMAZON_BEDROCK_METADATA"
                }
              }
            }' \
            --region "$CDK_DEFAULT_REGION" 2>&1)
          KB_EXIT_CODE=$?
          set -e  # Re-enable exit on error
          
          echo "Command completed."
          echo "ðŸ” Debug: KB creation exit code: $KB_EXIT_CODE"
          echo "ðŸ” Debug: KB creation output length: ${#KB_CREATE_OUTPUT}"
          echo "ðŸ” Debug: KB creation output: $KB_CREATE_OUTPUT"
          
          if [ $KB_EXIT_CODE -eq 0 ]; then
            KB_ID=$(echo "$KB_CREATE_OUTPUT" | jq -r '.knowledgeBase.knowledgeBaseId // empty')
            if [ -n "$KB_ID" ] && [ "$KB_ID" != "null" ] && [ "$KB_ID" != "empty" ]; then
              echo "âœ“ Knowledge Base created with supplemental storage: $KB_ID"
              echo "âœ“ Supplemental storage configured: s3://$SUPPLEMENTAL_BUCKET/"
              echo "âœ“ Ready for Bedrock Data Automation parsing of PDFs"
            else
              echo "âš  Knowledge Base creation succeeded but couldn't extract ID"
              echo "Full output: $KB_CREATE_OUTPUT"
              # Try to extract ID differently
              KB_ID=$(echo "$KB_CREATE_OUTPUT" | grep -o '"knowledgeBaseId":"[^"]*"' | cut -d'"' -f4)
              echo "Extracted KB ID: $KB_ID"
            fi
          else
            echo "âœ— Failed to create Knowledge Base with supplemental storage"
            echo "Exit code: $KB_EXIT_CODE"
            echo "Error output: $KB_CREATE_OUTPUT"
            echo "Collection ARN: $COLLECTION_ARN"
            echo "KB Role ARN: $KB_ROLE_ARN"
            echo "Supplemental Bucket: $SUPPLEMENTAL_BUCKET"
            exit 1
          fi
          
          # Step 6: Create Data Sources (S3 for PDFs + Web Crawler for Websites)
          echo "=== PHASE 4: Creating Dual Data Sources ==="
          BUCKET_ARN="arn:aws:s3:::${DOCUMENTS_BUCKET}"
          
          # Data Source 1: S3 for PDFs with Bedrock Data Automation Parser
          echo "Creating S3 Data Source for PDFs..."
          S3_DS_ID=$(aws bedrock-agent list-data-sources \
            --knowledge-base-id "$KB_ID" \
            --region "$CDK_DEFAULT_REGION" \
            --query "dataSourceSummaries[?name=='blood-centers-pdfs'].dataSourceId | [0]" \
            --output text 2>/dev/null || echo "")
          
          if [ -n "$S3_DS_ID" ] && [ "$S3_DS_ID" != "None" ] && [ "$S3_DS_ID" != "null" ]; then
            echo "âœ“ Found existing S3 Data Source: $S3_DS_ID"
          else
            echo "Creating new S3 Data Source for PDFs..."
            S3_DS_ID=$(aws bedrock-agent create-data-source \
              --name "blood-centers-pdfs" \
              --description "PDF documents about blood donation with Bedrock Data Automation parser" \
              --knowledge-base-id "$KB_ID" \
              --data-deletion-policy RETAIN \
              --data-source-configuration '{
                "type": "S3",
                "s3Configuration": {
                  "bucketArn": "'"$BUCKET_ARN"'",
                  "inclusionPrefixes": ["pdfs/"]
                }
              }' \
              --vector-ingestion-configuration '{
                "chunkingConfiguration": {
                  "chunkingStrategy": "FIXED_SIZE",
                  "fixedSizeChunkingConfiguration": {
                    "maxTokens": 300,
                    "overlapPercentage": 20
                  }
                },
                "parsingConfiguration": {
                  "parsingStrategy": "BEDROCK_DATA_AUTOMATION",
                  "bedrockDataAutomationConfiguration": {
                    "parsingModality": "MULTIMODAL"
                  }
                }
              }' \
              --region "$CDK_DEFAULT_REGION" \
              --query 'dataSource.dataSourceId' --output text)
            
            if [ -z "$S3_DS_ID" ] || [ "$S3_DS_ID" = "None" ]; then
              echo "âœ— Failed to create S3 Data Source"
              exit 1
            fi
            
            echo "âœ“ S3 Data Source created: $S3_DS_ID"
          fi
          
          # Data Source 2: Web Crawler for Websites
          echo "Creating Web Crawler Data Source..."
          WEB_DS_ID=$(aws bedrock-agent list-data-sources \
            --knowledge-base-id "$KB_ID" \
            --region "$CDK_DEFAULT_REGION" \
            --query "dataSourceSummaries[?name=='blood-centers-websites'].dataSourceId | [0]" \
            --output text 2>/dev/null || echo "")
          
          if [ -n "$WEB_DS_ID" ] && [ "$WEB_DS_ID" != "None" ] && [ "$WEB_DS_ID" != "null" ]; then
            echo "âœ“ Found existing Web Crawler Data Source: $WEB_DS_ID"
          else
            echo "Creating new Web Crawler Data Source..."
            
            # Read URLs from S3 files dynamically
            echo "Reading URLs from S3 files..."
            SEED_URLS=""
            
            # Read daily-sync.txt for daily URLs
            if aws s3 ls "s3://$DOCUMENTS_BUCKET/daily-sync.txt" >/dev/null 2>&1; then
              DAILY_URLS=$(aws s3 cp "s3://$DOCUMENTS_BUCKET/daily-sync.txt" - | grep -v '^#' | grep -v '^$')
              echo "Found daily sync URLs:"
              echo "$DAILY_URLS"
            fi
            
            # Read urls.txt for regular URLs
            if aws s3 ls "s3://$DOCUMENTS_BUCKET/urls.txt" >/dev/null 2>&1; then
              REGULAR_URLS=$(aws s3 cp "s3://$DOCUMENTS_BUCKET/urls.txt" - | grep -v '^#' | grep -v '^$')
              echo "Found regular URLs:"
              echo "$REGULAR_URLS"
            fi
            
            # Combine URLs and format for JSON
            ALL_URLS=$(printf "%s\n%s\n" "$DAILY_URLS" "$REGULAR_URLS" | grep -v '^$' | grep -v '^#' | sort -u)
            
            if [ -n "$ALL_URLS" ]; then
              # Convert URLs to JSON format (remove trailing comma properly)
              SEED_URLS=$(echo "$ALL_URLS" | sed 's/^/{"url": "/' | sed 's/$/"},/' | tr '\n' ' ')
              # Remove the trailing comma and space
              SEED_URLS=$(echo "$SEED_URLS" | sed 's/, *$//')
              echo "Formatted seed URLs: $SEED_URLS"
            else
              # Fallback to default URLs if files not found
              echo "âš  No URL files found, using default URLs"
              SEED_URLS='{"url": "https://americasblood.org/for-donors/americas-blood-supply/"},
                        {"url": "https://americasblood.org/for-donors/find-a-blood-center/"},
                        {"url": "https://americasblood.org/news/"},
                        {"url": "https://americasblood.org/newsroom/"},
                        {"url": "https://americasblood.org/one-pagers-faqs/"}'
            fi
            
            WEB_DS_ID=$(aws bedrock-agent create-data-source \
              --name "blood-centers-websites" \
              --description "Blood donation websites and news from America's Blood Centers" \
              --knowledge-base-id "$KB_ID" \
              --data-deletion-policy RETAIN \
              --data-source-configuration '{
                "type": "WEB",
                "webConfiguration": {
                  "sourceConfiguration": {
                    "urlConfiguration": {
                      "seedUrls": ['"$SEED_URLS"']
                    }
                  },
                  "crawlerConfiguration": {
                    "crawlerLimits": {
                      "rateLimit": 300,
                      "maxPages": 50
                    },
                    "exclusionFilters": [
                      ".*/wp-admin/.*", 
                      ".*/login/.*", 
                      ".*/admin/.*",
                      ".*/wp-content/uploads/.*"
                    ],
                    "inclusionFilters": [
                      ".*/for-donors/.*", 
                      ".*/news/.*", 
                      ".*/newsroom/.*",
                      ".*/one-pagers-faqs/.*"
                    ]
                  }
                }
              }' \
              --vector-ingestion-configuration '{
                "chunkingConfiguration": {
                  "chunkingStrategy": "FIXED_SIZE",
                  "fixedSizeChunkingConfiguration": {
                    "maxTokens": 300,
                    "overlapPercentage": 20
                  }
                },
                "parsingConfiguration": {
                  "parsingStrategy": "BEDROCK_DATA_AUTOMATION",
                  "bedrockDataAutomationConfiguration": {
                    "parsingModality": "MULTIMODAL"
                  }
                }
              }' \
              --region "$CDK_DEFAULT_REGION" \
              --query 'dataSource.dataSourceId' --output text)
            
            if [ -z "$WEB_DS_ID" ] || [ "$WEB_DS_ID" = "None" ]; then
              echo "âœ— Failed to create Web Crawler Data Source"
              exit 1
            fi
            
            echo "âœ“ Web Crawler Data Source created: $WEB_DS_ID"
          fi
          
          # Set primary data source ID for backward compatibility
          DS_ID="$S3_DS_ID"
          
          # Step 7: Document upload handled by CDK
          echo "=== PHASE 5: Document Upload ==="
          echo "âœ“ Documents already uploaded by CDK BucketDeployment"
          echo "âœ“ PDFs deployed to s3://$DOCUMENTS_BUCKET/pdfs/"
          echo "âœ“ Text files deployed to s3://$DOCUMENTS_BUCKET/ (root level)"
          
          echo "âœ“ Initial documents uploaded"
          
          # Step 8: Update Lambda environment variables
          echo "=== PHASE 6: Lambda Configuration ==="
          
          # Update chat Lambda
          CHAT_ENV=$(aws lambda get-function-configuration \
            --function-name "$CHAT_LAMBDA_NAME" \
            --region "$CDK_DEFAULT_REGION" \
            --query 'Environment.Variables' \
            --output json)
          
          echo "$CHAT_ENV" | jq \
            --arg kb_id "$KB_ID" \
            '{Variables: (. + {KNOWLEDGE_BASE_ID: $kb_id})}' > /tmp/chat_env.json
          
          aws lambda update-function-configuration \
            --function-name "$CHAT_LAMBDA_NAME" \
            --environment file:///tmp/chat_env.json \
            --region "$CDK_DEFAULT_REGION" >/dev/null
          
          # Update data ingestion Lambda
          DATA_ENV=$(aws lambda get-function-configuration \
            --function-name "$DATA_INGESTION_LAMBDA_NAME" \
            --region "$CDK_DEFAULT_REGION" \
            --query 'Environment.Variables' \
            --output json)
          
          echo "$DATA_ENV" | jq \
            --arg kb_id "$KB_ID" \
            --arg s3_ds_id "$S3_DS_ID" \
            --arg web_ds_id "$WEB_DS_ID" \
            '{Variables: (. + {KNOWLEDGE_BASE_ID: $kb_id, S3_DATA_SOURCE_ID: $s3_ds_id, WEB_DATA_SOURCE_ID: $web_ds_id, DATA_SOURCE_ID: $s3_ds_id})}' > /tmp/data_env.json
          
          aws lambda update-function-configuration \
            --function-name "$DATA_INGESTION_LAMBDA_NAME" \
            --environment file:///tmp/data_env.json \
            --region "$CDK_DEFAULT_REGION" >/dev/null
          
          echo "âœ“ Lambda environment variables updated"
          
          # Step 9: Deploy Frontend (before ingestion to save time)
          echo "=== PHASE 7: Frontend Deployment ==="
          
          # Check if we're in a CodeBuild environment and have access to the Frontend directory
          if [ -d "../Frontend" ]; then
            echo "Deploying React frontend to AWS Amplify..."
            
            # Debug: Show current directory and Frontend structure
            echo "ðŸ” Debug: Current directory: $(pwd)"
            echo "ðŸ” Debug: Checking Frontend directory..."
            ls -la ../Frontend/ || echo "Frontend directory not accessible"
            echo "ðŸ” Debug: Checking Frontend/public directory..."
            ls -la ../Frontend/public/ || echo "Frontend/public directory not accessible"
            echo "ðŸ” Debug: Checking for index.html specifically..."
            [ -f "../Frontend/public/index.html" ] && echo "âœ“ index.html found" || echo "âœ— index.html NOT found"
            
            # Additional debugging for file permissions and content
            echo "ðŸ” Debug: File permissions check..."
            ls -la ../Frontend/public/index.html 2>/dev/null || echo "Cannot access index.html file"
            echo "ðŸ” Debug: Directory permissions..."
            ls -ld ../Frontend/public/ 2>/dev/null || echo "Cannot access public directory"
            
            # Try to read the file to verify it's accessible
            echo "ðŸ” Debug: Attempting to read index.html..."
            head -5 ../Frontend/public/index.html 2>/dev/null && echo "âœ“ File is readable" || echo "âœ— File is not readable"
            
            # Create Amplify app
            AMPLIFY_APP_NAME="${PROJECT_NAME}-chatbot"
            echo "Creating Amplify app: $AMPLIFY_APP_NAME"
            
            # Check if app already exists
            EXISTING_APP_ID=$(aws amplify list-apps --query "apps[?name=='$AMPLIFY_APP_NAME'].appId | [0]" --output text 2>/dev/null || echo "")
            
            if [ -n "$EXISTING_APP_ID" ] && [ "$EXISTING_APP_ID" != "None" ] && [ "$EXISTING_APP_ID" != "null" ]; then
              echo "âœ“ Found existing Amplify app: $EXISTING_APP_ID"
              AMPLIFY_APP_ID="$EXISTING_APP_ID"
            else
              echo "Creating new Amplify app..."
              AMPLIFY_APP_ID=$(aws amplify create-app \
                --name "$AMPLIFY_APP_NAME" \
                --platform WEB \
                --environment-variables "REACT_APP_API_BASE_URL=$API_URL,REACT_APP_CHAT_ENDPOINT=$API_URL" \
                --query 'app.appId' --output text)
              echo "âœ“ Amplify app created: $AMPLIFY_APP_ID"
            fi
            
            # Create main branch if it doesn't exist
            EXISTING_BRANCH=$(aws amplify list-branches --app-id "$AMPLIFY_APP_ID" --query "branches[?branchName=='main'].branchName | [0]" --output text 2>/dev/null || echo "")
            
            if [ -z "$EXISTING_BRANCH" ] || [ "$EXISTING_BRANCH" = "None" ] || [ "$EXISTING_BRANCH" = "null" ]; then
              echo "Creating main branch..."
              aws amplify create-branch \
                --app-id "$AMPLIFY_APP_ID" \
                --branch-name "main" \
                --stage PRODUCTION
              echo "âœ“ Main branch created"
            else
              echo "âœ“ Main branch already exists"
            fi
            
            # Build and deploy frontend
            echo "Building React frontend..."
            cd ../Frontend
            
            # Ensure index.html exists and is accessible
            if [ ! -f "public/index.html" ] || [ ! -r "public/index.html" ]; then
              echo "âš  index.html missing or not readable, creating it..."
              mkdir -p public
              cat > public/index.html << EOF
<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="theme-color" content="#B71C1C" />
    <meta name="description" content="America's Blood Centers AI Assistant" />
    <link rel="manifest" href="%PUBLIC_URL%/manifest.json" />
    <title>America's Blood Centers - AI Assistant</title>
  </head>
  <body>
    <noscript>You need to enable JavaScript to run this app.</noscript>
    <div id="root"></div>
  </body>
</html>
EOF
              echo "âœ“ index.html created"
            else
              echo "âœ“ index.html exists and is readable"
            fi
            
            # Ensure manifest.json exists
            if [ ! -f "public/manifest.json" ]; then
              echo "âš  manifest.json missing, creating it..."
              cat > public/manifest.json << EOF
{
  "short_name": "ABC Assistant",
  "name": "America's Blood Centers AI Assistant",
  "start_url": ".",
  "display": "standalone",
  "theme_color": "#B71C1C",
  "background_color": "#ffffff"
}
EOF
              echo "âœ“ manifest.json created"
            fi
            
            # Create environment file with API URL
            echo "REACT_APP_API_BASE_URL=$API_URL" > .env.production
            echo "REACT_APP_CHAT_ENDPOINT=$API_URL" >> .env.production
            
            # Install dependencies and build
            echo "Installing npm dependencies..."
            npm install --production --no-audit --no-fund
            
            echo "Building React application..."
            set +e  # Temporarily disable exit on error
            BUILD_OUTPUT=$(npm run build 2>&1)
            BUILD_EXIT_CODE=$?
            set -e  # Re-enable exit on error
            
            echo "ðŸ” Debug: Build exit code: $BUILD_EXIT_CODE"
            
            if [ $BUILD_EXIT_CODE -eq 0 ]; then
              echo "âœ“ React build successful"
            else
              echo "âœ— React build failed"
              echo "Build output: $BUILD_OUTPUT"
              echo "ðŸ“‹ Attempting manual build recovery..."
              
              # Try to create a minimal build directory
              mkdir -p build
              cat > build/index.html << EOF
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>America's Blood Centers - AI Assistant</title>
</head>
<body>
    <div id="root">
        <h1>America's Blood Centers AI Assistant</h1>
        <p>The application is being deployed. Please check back shortly.</p>
        <p>API Endpoint: <code>$API_URL</code></p>
    </div>
</body>
</html>
EOF
              echo "âœ“ Created fallback build"
            fi
            
            # Create deployment package
            cd build
            zip -r ../build.zip .
            cd ..
            
            # Upload to S3 for Amplify deployment using CDK-created bucket
            BUILD_BUCKET=$(aws cloudformation describe-stacks \
              --stack-name "$STACK_NAME" \
              --query "Stacks[0].Outputs[?OutputKey=='BuildsBucketName'].OutputValue | [0]" \
              --output text)
            
            if [ -n "$BUILD_BUCKET" ] && [ "$BUILD_BUCKET" != "None" ]; then
              echo "Uploading build to CDK-created S3 bucket: $BUILD_BUCKET"
              BUILD_KEY="builds/build-$(date +%s).zip"
              aws s3 cp build.zip "s3://$BUILD_BUCKET/$BUILD_KEY"
              echo "âœ“ Build uploaded to S3"
            else
              echo "âš  Builds bucket not found, using fallback"
              BUILD_BUCKET="${PROJECT_NAME}-builds-${AWS_ACCOUNT_ID}-${CDK_DEFAULT_REGION}"
              BUILD_KEY="builds/build-$(date +%s).zip"
              aws s3 cp build.zip "s3://$BUILD_BUCKET/$BUILD_KEY" 2>/dev/null || echo "âš  Could not upload to S3"
            fi
            
            # Start Amplify deployment
            echo "Starting Amplify deployment..."
            JOB_ID=$(aws amplify start-deployment \
              --app-id "$AMPLIFY_APP_ID" \
              --branch-name "main" \
              --source-url "s3://$BUILD_BUCKET/$BUILD_KEY" \
              --query 'jobSummary.jobId' --output text 2>/dev/null || echo "MANUAL")
            
            if [ "$JOB_ID" != "MANUAL" ]; then
              echo "âœ“ Amplify deployment started: $JOB_ID"
              FRONTEND_URL="https://main.$AMPLIFY_APP_ID.amplifyapp.com"
              echo "âœ“ Frontend will be available at: $FRONTEND_URL"
            else
              echo "âš  Automatic deployment failed. Manual deployment required."
              echo "ðŸ“‹ Manual steps:"
              echo "1. Go to AWS Amplify Console"
              echo "2. Select app: $AMPLIFY_APP_NAME"
              echo "3. Deploy the build.zip file manually"
              FRONTEND_URL="https://main.$AMPLIFY_APP_ID.amplifyapp.com (manual deployment required)"
            fi
            
            cd ../Backend
          else
            echo "âš  Frontend directory not found. Skipping frontend deployment."
            echo "ðŸ“‹ To deploy frontend manually:"
            echo "1. Navigate to the Frontend directory"
            echo "2. Set REACT_APP_API_BASE_URL=$API_URL"
            echo "3. Run: npm install && npm run build"
            echo "4. Deploy to AWS Amplify"
            FRONTEND_URL="Manual deployment required"
          fi
          
          # Step 10: Start sequential ingestion jobs (PDFs first, then websites)
          echo ""
          echo "=== PHASE 8: Knowledge Base Ingestion ==="
          
          # Start S3 ingestion job for PDFs (faster, so we do this first)
          echo "Starting S3 ingestion job for PDFs..."
          S3_INGESTION_JOB_ID=$(aws bedrock-agent start-ingestion-job \
            --knowledge-base-id "$KB_ID" \
            --data-source-id "$S3_DS_ID" \
            --description "Initial ingestion of PDF documents" \
            --region "$CDK_DEFAULT_REGION" \
            --query 'ingestionJob.ingestionJobId' --output text)
          
          echo "âœ“ S3 ingestion job started: $S3_INGESTION_JOB_ID"
          
          # Wait for S3 ingestion to complete before starting web crawler
          echo "Waiting for S3 ingestion to complete before starting web crawler..."
          S3_JOB_STATUS="IN_PROGRESS"
          WAIT_TIME=0
          MAX_WAIT=600  # 10 minutes max wait
          
          while [ "$S3_JOB_STATUS" = "IN_PROGRESS" ] && [ $WAIT_TIME -lt $MAX_WAIT ]; do
            sleep 30
            WAIT_TIME=$((WAIT_TIME + 30))
            
            S3_JOB_STATUS=$(aws bedrock-agent get-ingestion-job \
              --knowledge-base-id "$KB_ID" \
              --data-source-id "$S3_DS_ID" \
              --ingestion-job-id "$S3_INGESTION_JOB_ID" \
              --region "$CDK_DEFAULT_REGION" \
              --query 'ingestionJob.status' --output text)
            
            echo "S3 ingestion status: $S3_JOB_STATUS (waited ${WAIT_TIME}s)"
          done
          
          if [ "$S3_JOB_STATUS" = "COMPLETE" ]; then
            echo "âœ“ S3 ingestion completed successfully"
            
            # Now start Web Crawler ingestion job
            echo "Starting Web Crawler ingestion job..."
            WEB_INGESTION_JOB_ID=$(aws bedrock-agent start-ingestion-job \
              --knowledge-base-id "$KB_ID" \
              --data-source-id "$WEB_DS_ID" \
              --description "Initial crawling of America's Blood Centers websites" \
              --region "$CDK_DEFAULT_REGION" \
              --query 'ingestionJob.ingestionJobId' --output text)
            
            echo "âœ“ Web Crawler ingestion job started: $WEB_INGESTION_JOB_ID"
          else
            echo "âš  S3 ingestion did not complete in time (status: $S3_JOB_STATUS)"
            echo "âš  Skipping web crawler ingestion for now"
            WEB_INGESTION_JOB_ID="SKIPPED"
          fi
          
          # Set primary ingestion job ID for backward compatibility
          INGESTION_JOB_ID="$S3_INGESTION_JOB_ID"
          
          # Step 11: Update Daily Sync Lambda (already created by CDK)
          echo ""
          echo "=== PHASE 9: Daily Sync Configuration ==="
          
          # Update the existing data ingestion Lambda with Knowledge Base IDs
          DATA_INGESTION_FUNCTION_NAME=$(aws cloudformation describe-stacks \
            --stack-name "$STACK_NAME" \
            --query "Stacks[0].Outputs[?OutputKey=='DataIngestionFunctionName'].OutputValue | [0]" \
            --output text)
          
          if [ -n "$DATA_INGESTION_FUNCTION_NAME" ] && [ "$DATA_INGESTION_FUNCTION_NAME" != "None" ]; then
            echo "Updating data ingestion Lambda function: $DATA_INGESTION_FUNCTION_NAME"
            
            # Update environment variables with actual Knowledge Base and Data Source IDs
            aws lambda update-function-configuration \
              --function-name "$DATA_INGESTION_FUNCTION_NAME" \
              --environment "Variables={DOCUMENTS_BUCKET=$DOCUMENTS_BUCKET,KNOWLEDGE_BASE_ID=$KB_ID,S3_DATA_SOURCE_ID=$S3_DS_ID,WEB_DATA_SOURCE_ID=$WEB_DS_ID}" \
              --region "$CDK_DEFAULT_REGION"
            
            echo "âœ“ Data ingestion Lambda updated with Knowledge Base IDs"
            echo "âœ“ Daily sync is already configured via CDK EventBridge rule"
            echo "âœ“ Schedule: Daily at 7 PM UTC (2 PM EST)"
            
            # Check if daily-sync.txt exists for URL configuration
            if aws s3 ls "s3://$DOCUMENTS_BUCKET/daily-sync.txt" >/dev/null 2>&1; then
              echo "âœ“ Found daily-sync.txt - URLs will be read dynamically"
            else
              echo "ðŸ“‹ To configure daily sync URLs:"
              echo "1. Upload daily-sync.txt to s3://$DOCUMENTS_BUCKET/"
              echo "2. Add URLs (one per line) that should be synced daily"
              echo "3. The Lambda function will read these URLs automatically"
            fi
          else
            echo "âš  Data ingestion Lambda function not found in stack outputs"
            echo "âš  Daily sync may not be properly configured"
          fi
          
          echo ""
          echo "========================================="
          echo "âœ… Deployment Complete!"
          echo "========================================="
          echo "ðŸ§  Knowledge Base ID: $KB_ID"
          echo "ðŸ“„ S3 Data Source ID: $S3_DS_ID (PDFs with Bedrock Data Automation)"
          echo "ðŸŒ Web Crawler Data Source ID: $WEB_DS_ID (Websites)"
          echo "ðŸ“ Documents Bucket: $DOCUMENTS_BUCKET"
          echo "ðŸ” Collection ARN: $COLLECTION_ARN"
          echo "ðŸ“Š S3 Ingestion Job ID: $S3_INGESTION_JOB_ID"
          echo "ðŸ•·ï¸ Web Ingestion Job ID: $WEB_INGESTION_JOB_ID"
          echo "ðŸŒ API Gateway URL: $API_URL"
          echo "ðŸ’» Frontend URL: ${FRONTEND_URL:-Not deployed}"
          echo "========================================="
          echo ""
          echo "========================================="
          echo "ðŸ“‹ Deployment Summary"
          echo "========================================="
          echo "ðŸ“ Documents Bucket: $DOCUMENTS_BUCKET"
          echo "ðŸ§  Knowledge Base ID: $KB_ID"
          echo "ðŸ“„ S3 Data Source ID: $S3_DS_ID (PDFs)"
          echo "ðŸŒ Web Data Source ID: $WEB_DS_ID (Websites)"
          echo "ðŸ”— API Gateway URL: $API_URL"
          echo ""
          echo "========================================="
          echo "ðŸš€ Next Steps"
          echo "========================================="
          echo "1ï¸âƒ£ Wait 5-10 minutes for ingestion jobs to complete"
          echo ""
          echo "2ï¸âƒ£ Test the API:"
          echo "   curl -X POST $API_URL \\"
          echo "     -H 'Content-Type: application/json' \\"
          echo "     -d '{\"message\":\"How many people donate blood?\",\"language\":\"en\"}'"
          echo ""
          echo "3ï¸âƒ£ Test Spanish support:"
          echo "   curl -X POST $API_URL \\"
          echo "     -H 'Content-Type: application/json' \\"
          echo "     -d '{\"message\":\"Â¿CuÃ¡ntas personas donan sangre?\",\"language\":\"es\"}'"
          echo ""
          echo "4ï¸âƒ£ Monitor ingestion jobs:"
          echo "   # S3 Data Source (PDFs):"
          echo "   aws bedrock-agent get-ingestion-job \\"
          echo "     --knowledge-base-id $KB_ID \\"
          echo "     --data-source-id $S3_DS_ID \\"
          echo "     --ingestion-job-id $S3_INGESTION_JOB_ID \\"
          echo "     --region $CDK_DEFAULT_REGION"
          echo ""
          echo "   # Web Crawler Data Source:"
          echo "   aws bedrock-agent get-ingestion-job \\"
          echo "     --knowledge-base-id $KB_ID \\"
          echo "     --data-source-id $WEB_DS_ID \\"
          echo "     --ingestion-job-id $WEB_INGESTION_JOB_ID \\"
          echo "     --region $CDK_DEFAULT_REGION"
          echo ""
          echo "5ï¸âƒ£ Add more documents:"
          echo "   # Add PDFs (uses Bedrock Data Automation parser):"
          echo "   aws s3 cp document.pdf s3://$DOCUMENTS_BUCKET/pdfs/"
          echo "   aws bedrock-agent start-ingestion-job \\"
          echo "     --knowledge-base-id $KB_ID \\"
          echo "     --data-source-id $S3_DS_ID \\"
          echo "     --region $CDK_DEFAULT_REGION"
          echo ""
          echo "   # Re-crawl websites (automatic parsing):"
          echo "   aws bedrock-agent start-ingestion-job \\"
          echo "     --knowledge-base-id $KB_ID \\"
          echo "     --data-source-id $WEB_DS_ID \\"
          echo "     --region $CDK_DEFAULT_REGION"
          echo ""
          echo "ðŸ“Š Monitor resources:"
          echo "  â€¢ Knowledge Base: https://console.aws.amazon.com/bedrock/home?region=${CDK_DEFAULT_REGION}#/knowledge-bases"
          echo "  â€¢ OpenSearch: https://console.aws.amazon.com/aos/home?region=${CDK_DEFAULT_REGION}#opensearch/collections"
          echo "  â€¢ S3 Bucket: https://s3.console.aws.amazon.com/s3/buckets/$DOCUMENTS_BUCKET"
          echo ""
          echo "ðŸ’° Estimated monthly cost: \$8-20 (vs \$20+ for Q Business)"
          echo "ðŸŽ¯ Features: Dual data sources, bilingual support, automatic web crawling"
          echo ""
        fi

  post_build:
    commands:
      - echo "========================================="
      - echo "ðŸŽ‰ Deployment Complete"
      - echo "========================================="
      - |
        if [ "$ACTION" = "deploy" ]; then
          echo "âœ… Bedrock chatbot deployed successfully"
          echo "ðŸ“Š Check CloudWatch Logs for Lambda execution details"
          echo "ðŸ§  Test Knowledge Base retrieval via the chat API"
          echo "ðŸŒ Monitor ingestion jobs in Bedrock console"
        else
          echo "âœ… Stack destroyed successfully"
        fi

artifacts:
  files:
    - "**/*"
  base-directory: "Backend/cdk.out"